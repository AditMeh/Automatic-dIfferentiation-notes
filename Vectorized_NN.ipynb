{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vectorized NN",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsquXYSMArqUkEHIiMDT5U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AditMeh/deep-learning/blob/main/Vectorized_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea6j52F0R95W"
      },
      "source": [
        "import numpy as np\r\n",
        "def compute_dw(delta, a):\r\n",
        "        batch_size, num_prev_neurons = a.shape\r\n",
        "        _, num_curr_neurons = delta.shape\r\n",
        "\r\n",
        "        tile_delta = np.tile(delta, (1, num_prev_neurons)).reshape((batch_size, num_prev_neurons, num_curr_neurons))\r\n",
        "        tile_delta = np.transpose(tile_delta, (0, 2, 1))\r\n",
        "\r\n",
        "\r\n",
        "        tile_activations = np.tile(a, (1, num_curr_neurons)).reshape(batch_size, num_curr_neurons, num_prev_neurons)\r\n",
        "\r\n",
        "\r\n",
        "        dW =  tile_delta * tile_activations \r\n",
        "        dW = np.transpose(dW, (0, 2, 1))\r\n",
        "        return dW"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN03nBpqQNrg"
      },
      "source": [
        "\r\n",
        "class NeuralNet:\r\n",
        "    def __init__(self, layer_list):\r\n",
        "        self.layers = len(layer_list)\r\n",
        "        self.weights = []\r\n",
        "        self.biases = []\r\n",
        "\r\n",
        "\r\n",
        "        #randomly initialize weights and biases (scale each random value by 0.01 to prevent vanishing gradient due the saturation of sigmoid)\r\n",
        "        for layer_index in range(1, len(layer_list)):\r\n",
        "            self.weights.append(np.random.rand(layer_list[layer_index - 1], layer_list[layer_index])*0.01)\r\n",
        "            self.biases.append(np.random.rand(layer_list[layer_index],)*0.01)\r\n",
        "        \r\n",
        "        print(\"Weights\")\r\n",
        "        print([element.shape for element in self.weights])\r\n",
        "\r\n",
        "        print(\"\\n\" + \"Biases\")\r\n",
        "        print([element.shape for element in self.biases])\r\n",
        "    \r\n",
        "    def sigmoid(self, x):\r\n",
        "        return 1/(1 + (np.e)**(-x))\r\n",
        "    def sigmoid_prime(self, x):\r\n",
        "        return self.sigmoid(x)*(1 - self.sigmoid(x))\r\n",
        "    def compute_cost(self, real, predicted, batch_size):\r\n",
        "        return (1/batch_size) * (np.sum(1/2*(predicted - real)**2))\r\n",
        "    def cost_derivative(self, real, predicted):\r\n",
        "        return (predicted - real)\r\n",
        "    \r\n",
        "    def feedforward(self, x, output):\r\n",
        "       self.x = x\r\n",
        "       self.output = output\r\n",
        "\r\n",
        "       #Activation of the input layer is equivalent to the inputs that are passed in\r\n",
        "       current_a = self.x\r\n",
        "\r\n",
        "       self.z = []\r\n",
        "       self.a = [self.x]\r\n",
        "       for w_i, b_i in zip(self.weights, self.biases):\r\n",
        "           z_i = np.dot(current_a, w_i) + b_i\r\n",
        "\r\n",
        "           self.z.append(z_i)\r\n",
        "           current_a = self.sigmoid(z_i)\r\n",
        "           self.a.append(current_a)\r\n",
        "\r\n",
        "       return self.compute_cost(output, current_a, self.x.shape[0])\r\n",
        "    def backward(self):\r\n",
        "        #print([thing.shape for thing in self.a])\r\n",
        "        w_i_grad = [np.zeros(element.shape) for element in self.weights]\r\n",
        "        b_i_grad = [np.zeros(element.shape) for element in self.biases]\r\n",
        "\r\n",
        "        # compute final layer error\r\n",
        "        delta  = self.cost_derivative(self.output, (self.a)[-1]) * self.sigmoid_prime((self.z)[-1])\r\n",
        "\r\n",
        "        w_i_grad[-1] = compute_dw(delta, self.a[-2])\r\n",
        "        b_i_grad[-1] = delta\r\n",
        "        # compute  \r\n",
        "        for i in range(2, self.layers):\r\n",
        "            z_current = self.z[-i]\r\n",
        "\r\n",
        "            # delta non-final layer error\r\n",
        "\r\n",
        "            delta = np.dot(delta, (self.weights[-i + 1]).T) * self.sigmoid_prime(z_current)\r\n",
        "            #print(delta.shape)\r\n",
        "\r\n",
        "            # Computing dC/Dw\r\n",
        "\r\n",
        "            dW = compute_dw(delta, self.a[-i - 1])\r\n",
        "\r\n",
        "            w_i_grad[-i] = dW\r\n",
        "            b_i_grad[-i] = delta\r\n",
        "\r\n",
        "\r\n",
        "        return w_i_grad, b_i_grad\r\n",
        "    \r\n",
        "    def update_weights_and_biases(self, w_grad, b_grad, learning_rate):        \r\n",
        "\r\n",
        "        batch_size = w_grad[0].shape[0]\r\n",
        "        #apply gradients to current weights\r\n",
        "        for i in range(len(self.weights)): \r\n",
        "            self.weights[i] -= learning_rate*(np.sum(w_grad[i], axis = 0)/batch_size)\r\n",
        "            self.biases[i] -= learning_rate*(np.sum(b_grad[i], axis = 0)/batch_size)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0SMJOOsQnIq",
        "outputId": "18e69c44-5220-4206-d977-c34a8f2523d7"
      },
      "source": [
        "input = np.ones((10, 50))\r\n",
        "\r\n",
        "\r\n",
        "output = np.ones((10, 1))\r\n",
        "\r\n",
        "nn = NeuralNet([50, 20, 30, 2])\r\n",
        "\r\n",
        "nn.feedforward(input, output)\r\n",
        "nn.backward()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights\n",
            "[(50, 20), (20, 30), (30, 2)]\n",
            "\n",
            "Biases\n",
            "[(20,), (30,), (2,)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([(10, 50, 20), (10, 20, 30), (10, 30, 2)], [(10, 20), (10, 30), (10, 2)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viyDbUrxSAhp",
        "outputId": "c8406684-aa91-47ff-d51a-ae86107cf984"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\r\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\r\n",
        "\r\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2]) / 255\r\n",
        "\r\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\r\n",
        "\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "\r\n",
        "def prepare_mini_batches(minibatch_size: int):\r\n",
        "    \"\"\"\r\n",
        "    Takes in a minibatch size parameter and generates random indexes from the\r\n",
        "    training set (X_train). Returns a list of indexes of length minibatch_size.\r\n",
        "    \"\"\"\r\n",
        "    random_indexes = np.random.choice(X_train.shape[0], minibatch_size, replace=False)\r\n",
        "    return random_indexes\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# Creating one hot encoder\r\n",
        "encoder = OneHotEncoder()\r\n",
        "encoder.fit(Y_train.reshape(-1, 1))\r\n",
        "\r\n",
        "\r\n",
        "nn = NeuralNet([784, 362, 100, 10])\r\n",
        "\r\n",
        "#training loop\r\n",
        "epochs = 50\r\n",
        "mini_size = 32\r\n",
        "lr = 0.01\r\n",
        "\r\n",
        "for i in range(epochs):\r\n",
        "\r\n",
        "    print(\"EPOCH: \" + str(i + 1))\r\n",
        "\r\n",
        "    #Generates a randomly chosen set of training indexes per epoch\r\n",
        "    random_indexes = prepare_mini_batches(mini_size)\r\n",
        "    cost_iter = nn.feedforward(X_train[random_indexes], encoder.transform(Y_train[random_indexes].reshape(-1, 1)).toarray())\r\n",
        "    print(cost_iter)\r\n",
        "    grads_w, grads_b = nn.backward()\r\n",
        "\r\n",
        "    nn.update_weights_and_biases(grads_w, grads_b, learning_rate=lr)\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights\n",
            "[(784, 362), (362, 100), (100, 10)]\n",
            "\n",
            "Biases\n",
            "[(362,), (100,), (10,)]\n",
            "EPOCH: 1\n",
            "1.6731024220610091\n",
            "EPOCH: 2\n",
            "1.5950638754436333\n",
            "EPOCH: 3\n",
            "1.5170748465424986\n",
            "EPOCH: 4\n",
            "1.4417938946739213\n",
            "EPOCH: 5\n",
            "1.3720806144030686\n",
            "EPOCH: 6\n",
            "1.3083736597711497\n",
            "EPOCH: 7\n",
            "1.2451773951446292\n",
            "EPOCH: 8\n",
            "1.1891888293997566\n",
            "EPOCH: 9\n",
            "1.1390783754304312\n",
            "EPOCH: 10\n",
            "1.0873418184531585\n",
            "EPOCH: 11\n",
            "1.043464803219305\n",
            "EPOCH: 12\n",
            "1.0015507013468286\n",
            "EPOCH: 13\n",
            "0.9643101019944917\n",
            "EPOCH: 14\n",
            "0.9299562413669962\n",
            "EPOCH: 15\n",
            "0.8964673678843496\n",
            "EPOCH: 16\n",
            "0.8678009361226471\n",
            "EPOCH: 17\n",
            "0.8436526652873632\n",
            "EPOCH: 18\n",
            "0.8160708517717126\n",
            "EPOCH: 19\n",
            "0.7917850022967567\n",
            "EPOCH: 20\n",
            "0.7741077400026235\n",
            "EPOCH: 21\n",
            "0.7525998391819165\n",
            "EPOCH: 22\n",
            "0.7351437315596149\n",
            "EPOCH: 23\n",
            "0.7208435933893589\n",
            "EPOCH: 24\n",
            "0.7028542948384227\n",
            "EPOCH: 25\n",
            "0.6913736934932853\n",
            "EPOCH: 26\n",
            "0.6758103746392391\n",
            "EPOCH: 27\n",
            "0.6666772715910146\n",
            "EPOCH: 28\n",
            "0.6528511139830022\n",
            "EPOCH: 29\n",
            "0.6417076440128607\n",
            "EPOCH: 30\n",
            "0.6332880327484706\n",
            "EPOCH: 31\n",
            "0.622358490353623\n",
            "EPOCH: 32\n",
            "0.6159571917075448\n",
            "EPOCH: 33\n",
            "0.6051858761985329\n",
            "EPOCH: 34\n",
            "0.6022863739245399\n",
            "EPOCH: 35\n",
            "0.592554224872865\n",
            "EPOCH: 36\n",
            "0.5871048914695732\n",
            "EPOCH: 37\n",
            "0.5808819725512531\n",
            "EPOCH: 38\n",
            "0.5740634828262718\n",
            "EPOCH: 39\n",
            "0.5707865005720913\n",
            "EPOCH: 40\n",
            "0.5642748394490142\n",
            "EPOCH: 41\n",
            "0.5599635031557377\n",
            "EPOCH: 42\n",
            "0.5553323292488779\n",
            "EPOCH: 43\n",
            "0.5497426876255598\n",
            "EPOCH: 44\n",
            "0.5479723283597884\n",
            "EPOCH: 45\n",
            "0.54344149080083\n",
            "EPOCH: 46\n",
            "0.5399915367806893\n",
            "EPOCH: 47\n",
            "0.5354103520467588\n",
            "EPOCH: 48\n",
            "0.5325745371538836\n",
            "EPOCH: 49\n",
            "0.5294284522181416\n",
            "EPOCH: 50\n",
            "0.5261611067776679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX0ehHSy_HI3",
        "outputId": "e7c50389-831b-4506-9c32-0ee302a2ab79"
      },
      "source": [
        "## Axis are reversed, review what this operation actually needs in ortder\r\n",
        "\r\n",
        "\r\n",
        "delta = np.array([[1, 2, 3, 4],\r\n",
        "                 [5, 6, 7, 8]])\r\n",
        "\r\n",
        "a = np.array([[5, 5],\r\n",
        "             [6, 6]])\r\n",
        "\r\n",
        "batch_size, num_prev_neurons = a.shape\r\n",
        "_, num_curr_neurons = delta.shape\r\n",
        "\r\n",
        "delta = np.tile(delta, (1, num_prev_neurons)).reshape((batch_size, num_prev_neurons, num_curr_neurons))\r\n",
        "\r\n",
        "\r\n",
        "delta = np.transpose(delta, (0, 2, 1))\r\n",
        "\r\n",
        "#print(delta)\r\n",
        "\r\n",
        "\r\n",
        "a = np.tile(a, (1, num_curr_neurons)).reshape(batch_size, num_curr_neurons, num_prev_neurons)\r\n",
        "\r\n",
        "#print(a)\r\n",
        "\r\n",
        "\r\n",
        "k = (delta * a)\r\n",
        "\r\n",
        "print(k)\r\n",
        "print(np.transpose(k, (0, 2, 1)).shape)\r\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[ 5  5]\n",
            "  [10 10]\n",
            "  [15 15]\n",
            "  [20 20]]\n",
            "\n",
            " [[30 30]\n",
            "  [36 36]\n",
            "  [42 42]\n",
            "  [48 48]]]\n",
            "(2, 2, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gr5m8r0WEy4",
        "outputId": "db31a189-77e8-4d1e-ec21-97dca9981574"
      },
      "source": [
        "a = np.array([[1,2,3,4,5],\r\n",
        "              [1,2,3,4,5]])\r\n",
        "\r\n",
        "p = np.array([1,2,3,4,4])\r\n",
        "print(p.shape)\r\n",
        "print(a - p)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5,)\n",
            "[[0 0 0 0 1]\n",
            " [0 0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0rLMAK-WZZ2",
        "outputId": "87b68efd-8ade-42d4-e55f-e1f5723ecf7c"
      },
      "source": [
        "a = np.array([\r\n",
        "              [[1,2,3,4,3],\r\n",
        "              [1,2,3,4,5]],\r\n",
        "              [[1,2,3,4,5],\r\n",
        "               [0,0,1,1,1]]\r\n",
        "              ])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "print(np.sum(a, axis= 1))\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 4 6 8 8]\n",
            " [1 2 4 5 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}