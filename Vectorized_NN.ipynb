{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vectorized NN",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjvFnZsVfs45ZH2SAqIw5b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AditMeh/deep-learning/blob/main/Vectorized_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea6j52F0R95W"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "def compute_dw(delta, a):\r\n",
        "    \"\"\"\r\n",
        "    This function contains a vectorized implementation of computing the\r\n",
        "    gradients of the weights given the error (delta) of layer L and the\r\n",
        "    activations of layer L-1\r\n",
        "    \"\"\"\r\n",
        "    num_items = (delta.shape[0])\r\n",
        "\r\n",
        "    dW = (1/num_items) * np.matmul(delta.T, a).T\r\n",
        "\r\n",
        "    return dW"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN03nBpqQNrg"
      },
      "source": [
        "class NeuralNet:\r\n",
        "    def __init__(self, layer_list):\r\n",
        "        self.layers = len(layer_list)\r\n",
        "        self.weights = []\r\n",
        "        self.biases = []\r\n",
        "\r\n",
        "\r\n",
        "        #randomly initialize weights and biases (scale each random value by 0.01 to prevent vanishing gradient due the saturation of sigmoid)\r\n",
        "        for layer_index in range(1, len(layer_list)):\r\n",
        "            self.weights.append(np.random.rand(layer_list[layer_index - 1], layer_list[layer_index])*0.01)\r\n",
        "            self.biases.append(np.random.rand(layer_list[layer_index],)*0.01)\r\n",
        "        \r\n",
        "        print(\"Weights\")\r\n",
        "        print([element.shape for element in self.weights])\r\n",
        "\r\n",
        "        print(\"\\n\" + \"Biases\")\r\n",
        "        print([element.shape for element in self.biases])\r\n",
        "    \r\n",
        "    def sigmoid(self, x):\r\n",
        "        return 1/(1 + (np.e)**(-x))\r\n",
        "    def sigmoid_prime(self, x):\r\n",
        "        return self.sigmoid(x)*(1 - self.sigmoid(x))\r\n",
        "    def compute_cost(self, real, predicted, batch_size):\r\n",
        "        return (1/batch_size) * (np.sum(1/2*(predicted - real)**2))\r\n",
        "    def cost_derivative(self, real, predicted):\r\n",
        "        return (predicted - real)\r\n",
        "    \r\n",
        "    def feedforward(self, x, output):\r\n",
        "       self.x = x\r\n",
        "       self.output = output\r\n",
        "\r\n",
        "       #Activation of the input layer is equivalent to the inputs that are passed in\r\n",
        "       current_a = self.x\r\n",
        "\r\n",
        "       self.z = []\r\n",
        "       self.a = [self.x]\r\n",
        "       for w_i, b_i in zip(self.weights, self.biases):\r\n",
        "           z_i = np.dot(current_a, w_i) + b_i\r\n",
        "\r\n",
        "           self.z.append(z_i)\r\n",
        "           current_a = self.sigmoid(z_i)\r\n",
        "           self.a.append(current_a)\r\n",
        "\r\n",
        "       return self.compute_cost(output, current_a, self.x.shape[0])\r\n",
        "    def backward(self):\r\n",
        "        #print([thing.shape for thing in self.a])\r\n",
        "        w_i_grad = [np.zeros(element.shape) for element in self.weights]\r\n",
        "        b_i_grad = [np.zeros(element.shape) for element in self.biases]\r\n",
        "\r\n",
        "        # compute final layer error\r\n",
        "        delta  = self.cost_derivative(self.output, (self.a)[-1]) * self.sigmoid_prime((self.z)[-1])\r\n",
        "\r\n",
        "        w_i_grad[-1] = compute_dw(delta, self.a[-2])\r\n",
        "        b_i_grad[-1] = delta\r\n",
        "        # compute  \r\n",
        "        for i in range(2, self.layers):\r\n",
        "            z_current = self.z[-i]\r\n",
        "\r\n",
        "            # delta non-final layer error\r\n",
        "\r\n",
        "            delta = np.dot(delta, (self.weights[-i + 1]).T) * self.sigmoid_prime(z_current)\r\n",
        "            #print(delta.shape)\r\n",
        "\r\n",
        "            # Computing dC/Dw\r\n",
        "\r\n",
        "            dW = compute_dw(delta, self.a[-i - 1])\r\n",
        "\r\n",
        "            w_i_grad[-i] = dW\r\n",
        "            b_i_grad[-i] = delta\r\n",
        "\r\n",
        "\r\n",
        "        return w_i_grad, b_i_grad\r\n",
        "    \r\n",
        "    def update_weights_and_biases(self, w_grad, b_grad, learning_rate):        \r\n",
        "\r\n",
        "        batch_size = w_grad[0].shape[0]\r\n",
        "        #apply gradients to current weights\r\n",
        "        for i in range(len(self.weights)): \r\n",
        "            self.weights[i] -= learning_rate*(np.sum(w_grad[i], axis = 0)/batch_size)\r\n",
        "            self.biases[i] -= learning_rate*(np.sum(b_grad[i], axis = 0)/batch_size)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0SMJOOsQnIq",
        "outputId": "bca01b72-9828-4f20-e675-9d576dd787e5"
      },
      "source": [
        "\"\"\"\r\n",
        "Test run\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "input = np.ones((10, 50))\r\n",
        "\r\n",
        "output = np.ones((10, 1))\r\n",
        "\r\n",
        "nn = NeuralNet([50, 20, 30, 2])\r\n",
        "\r\n",
        "nn.feedforward(input, output)\r\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights\n",
            "[(50, 20), (20, 30), (30, 2)]\n",
            "\n",
            "Biases\n",
            "[(20,), (30,), (2,)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.23219474824897107"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viyDbUrxSAhp",
        "outputId": "cc8fc3a1-75d3-459c-e47c-146e5119ce1c"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\r\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\r\n",
        "\r\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2]) / 255\r\n",
        "\r\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\r\n",
        "\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "\r\n",
        "def prepare_mini_batches(minibatch_size: int):\r\n",
        "    \"\"\"\r\n",
        "    Takes in a minibatch size parameter and generates random indexes from the\r\n",
        "    training set (X_train). Returns a list of indexes of length minibatch_size.\r\n",
        "    \"\"\"\r\n",
        "    random_indexes = np.random.choice(X_train.shape[0], minibatch_size, replace=False)\r\n",
        "    return random_indexes\r\n",
        "\r\n",
        "# Creating one hot encoder\r\n",
        "encoder = OneHotEncoder()\r\n",
        "encoder.fit(Y_train.reshape(-1, 1))\r\n",
        "\r\n",
        "\r\n",
        "#Initializing the neural net\r\n",
        "nn = NeuralNet([784, 362, 100, 10])\r\n",
        "\r\n",
        "#training loop\r\n",
        "epochs = 300\r\n",
        "mini_size = 32\r\n",
        "lr = 0.01\r\n",
        "\r\n",
        "\r\n",
        "history = []\r\n",
        "for i in range(epochs):\r\n",
        "\r\n",
        "    print(\"EPOCH: \" + str(i + 1))\r\n",
        "\r\n",
        "    #Generates a randomly chosen set of training indexes per epoch\r\n",
        "    random_indexes = prepare_mini_batches(mini_size)\r\n",
        "    cost_iter = nn.feedforward(X_train[random_indexes], encoder.transform(Y_train[random_indexes].reshape(-1, 1)).toarray())\r\n",
        "    history.append(cost_iter)\r\n",
        "    print(\"Cost: \" + str(cost_iter))\r\n",
        "    grads_w, grads_b = nn.backward()\r\n",
        "\r\n",
        "    nn.update_weights_and_biases(grads_w, grads_b, learning_rate=lr)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights\n",
            "[(784, 362), (362, 100), (100, 10)]\n",
            "\n",
            "Biases\n",
            "[(362,), (100,), (10,)]\n",
            "EPOCH: 1\n",
            "Cost: 1.6957720750604754\n",
            "EPOCH: 2\n",
            "Cost: 1.6809671190943622\n",
            "EPOCH: 3\n",
            "Cost: 1.6711356634740318\n",
            "EPOCH: 4\n",
            "Cost: 1.6620518006342995\n",
            "EPOCH: 5\n",
            "Cost: 1.651708404227719\n",
            "EPOCH: 6\n",
            "Cost: 1.6408712158492886\n",
            "EPOCH: 7\n",
            "Cost: 1.6295257301439228\n",
            "EPOCH: 8\n",
            "Cost: 1.6190329805721695\n",
            "EPOCH: 9\n",
            "Cost: 1.6099087450681593\n",
            "EPOCH: 10\n",
            "Cost: 1.601525314934362\n",
            "EPOCH: 11\n",
            "Cost: 1.5899864413045353\n",
            "EPOCH: 12\n",
            "Cost: 1.5816358086743336\n",
            "EPOCH: 13\n",
            "Cost: 1.571413985903932\n",
            "EPOCH: 14\n",
            "Cost: 1.5614921104477908\n",
            "EPOCH: 15\n",
            "Cost: 1.5494157662486585\n",
            "EPOCH: 16\n",
            "Cost: 1.5402195223862587\n",
            "EPOCH: 17\n",
            "Cost: 1.5323401322682415\n",
            "EPOCH: 18\n",
            "Cost: 1.521435470769471\n",
            "EPOCH: 19\n",
            "Cost: 1.5116012069888702\n",
            "EPOCH: 20\n",
            "Cost: 1.502734861917578\n",
            "EPOCH: 21\n",
            "Cost: 1.4926096820677737\n",
            "EPOCH: 22\n",
            "Cost: 1.484115801896817\n",
            "EPOCH: 23\n",
            "Cost: 1.4730896667546163\n",
            "EPOCH: 24\n",
            "Cost: 1.466146693802945\n",
            "EPOCH: 25\n",
            "Cost: 1.4548521637752678\n",
            "EPOCH: 26\n",
            "Cost: 1.4472359213328885\n",
            "EPOCH: 27\n",
            "Cost: 1.4380434569526357\n",
            "EPOCH: 28\n",
            "Cost: 1.4304082751220635\n",
            "EPOCH: 29\n",
            "Cost: 1.4206869964480382\n",
            "EPOCH: 30\n",
            "Cost: 1.4108025464166984\n",
            "EPOCH: 31\n",
            "Cost: 1.4016383997705348\n",
            "EPOCH: 32\n",
            "Cost: 1.3952217787072057\n",
            "EPOCH: 33\n",
            "Cost: 1.3856393646401104\n",
            "EPOCH: 34\n",
            "Cost: 1.37697655765441\n",
            "EPOCH: 35\n",
            "Cost: 1.3702400135868507\n",
            "EPOCH: 36\n",
            "Cost: 1.3601276029844254\n",
            "EPOCH: 37\n",
            "Cost: 1.3525700006739774\n",
            "EPOCH: 38\n",
            "Cost: 1.3448883220535603\n",
            "EPOCH: 39\n",
            "Cost: 1.3367356294662605\n",
            "EPOCH: 40\n",
            "Cost: 1.3280748349380422\n",
            "EPOCH: 41\n",
            "Cost: 1.321490767007734\n",
            "EPOCH: 42\n",
            "Cost: 1.312237578550305\n",
            "EPOCH: 43\n",
            "Cost: 1.30386232219641\n",
            "EPOCH: 44\n",
            "Cost: 1.2970597826000656\n",
            "EPOCH: 45\n",
            "Cost: 1.2892239131073384\n",
            "EPOCH: 46\n",
            "Cost: 1.2800881393387114\n",
            "EPOCH: 47\n",
            "Cost: 1.2731894269150041\n",
            "EPOCH: 48\n",
            "Cost: 1.2658399491826535\n",
            "EPOCH: 49\n",
            "Cost: 1.2599142842888331\n",
            "EPOCH: 50\n",
            "Cost: 1.2520801460366462\n",
            "EPOCH: 51\n",
            "Cost: 1.2436549723749246\n",
            "EPOCH: 52\n",
            "Cost: 1.238454221615692\n",
            "EPOCH: 53\n",
            "Cost: 1.2296199258864577\n",
            "EPOCH: 54\n",
            "Cost: 1.2230986308324927\n",
            "EPOCH: 55\n",
            "Cost: 1.2151261344600033\n",
            "EPOCH: 56\n",
            "Cost: 1.2079058967819378\n",
            "EPOCH: 57\n",
            "Cost: 1.2011326655462962\n",
            "EPOCH: 58\n",
            "Cost: 1.1966842287769428\n",
            "EPOCH: 59\n",
            "Cost: 1.189297168091786\n",
            "EPOCH: 60\n",
            "Cost: 1.1824468891674462\n",
            "EPOCH: 61\n",
            "Cost: 1.1739979003647631\n",
            "EPOCH: 62\n",
            "Cost: 1.1676512921483755\n",
            "EPOCH: 63\n",
            "Cost: 1.1599833112965023\n",
            "EPOCH: 64\n",
            "Cost: 1.1557086632183475\n",
            "EPOCH: 65\n",
            "Cost: 1.14818531228474\n",
            "EPOCH: 66\n",
            "Cost: 1.1430093832031836\n",
            "EPOCH: 67\n",
            "Cost: 1.1357900006870096\n",
            "EPOCH: 68\n",
            "Cost: 1.1321681535393808\n",
            "EPOCH: 69\n",
            "Cost: 1.1229694980588303\n",
            "EPOCH: 70\n",
            "Cost: 1.1174714310768514\n",
            "EPOCH: 71\n",
            "Cost: 1.1116495298081435\n",
            "EPOCH: 72\n",
            "Cost: 1.1059551332862658\n",
            "EPOCH: 73\n",
            "Cost: 1.0999692787658852\n",
            "EPOCH: 74\n",
            "Cost: 1.0949613965030571\n",
            "EPOCH: 75\n",
            "Cost: 1.0884836969796792\n",
            "EPOCH: 76\n",
            "Cost: 1.0830682489446837\n",
            "EPOCH: 77\n",
            "Cost: 1.0767859337937495\n",
            "EPOCH: 78\n",
            "Cost: 1.070664490185473\n",
            "EPOCH: 79\n",
            "Cost: 1.0660263272306514\n",
            "EPOCH: 80\n",
            "Cost: 1.0606990597547894\n",
            "EPOCH: 81\n",
            "Cost: 1.0542254869137166\n",
            "EPOCH: 82\n",
            "Cost: 1.0495946524383757\n",
            "EPOCH: 83\n",
            "Cost: 1.0444900703038065\n",
            "EPOCH: 84\n",
            "Cost: 1.038002392145434\n",
            "EPOCH: 85\n",
            "Cost: 1.0338570039561166\n",
            "EPOCH: 86\n",
            "Cost: 1.0277613390271303\n",
            "EPOCH: 87\n",
            "Cost: 1.0231904871140078\n",
            "EPOCH: 88\n",
            "Cost: 1.0183117740060952\n",
            "EPOCH: 89\n",
            "Cost: 1.014329817417107\n",
            "EPOCH: 90\n",
            "Cost: 1.0093412606530174\n",
            "EPOCH: 91\n",
            "Cost: 1.0036424015804044\n",
            "EPOCH: 92\n",
            "Cost: 0.9977565983258782\n",
            "EPOCH: 93\n",
            "Cost: 0.993077801934259\n",
            "EPOCH: 94\n",
            "Cost: 0.9883115057738164\n",
            "EPOCH: 95\n",
            "Cost: 0.9830040018686375\n",
            "EPOCH: 96\n",
            "Cost: 0.9787069304399697\n",
            "EPOCH: 97\n",
            "Cost: 0.9756245604521667\n",
            "EPOCH: 98\n",
            "Cost: 0.9697748737682248\n",
            "EPOCH: 99\n",
            "Cost: 0.9652435896972206\n",
            "EPOCH: 100\n",
            "Cost: 0.9613494824238551\n",
            "EPOCH: 101\n",
            "Cost: 0.9559863985468195\n",
            "EPOCH: 102\n",
            "Cost: 0.9504099682330718\n",
            "EPOCH: 103\n",
            "Cost: 0.9458850263154062\n",
            "EPOCH: 104\n",
            "Cost: 0.9427946447682634\n",
            "EPOCH: 105\n",
            "Cost: 0.9386755980728971\n",
            "EPOCH: 106\n",
            "Cost: 0.9358471682417169\n",
            "EPOCH: 107\n",
            "Cost: 0.9308090283034636\n",
            "EPOCH: 108\n",
            "Cost: 0.9247470209541101\n",
            "EPOCH: 109\n",
            "Cost: 0.9224108592591161\n",
            "EPOCH: 110\n",
            "Cost: 0.918548057621837\n",
            "EPOCH: 111\n",
            "Cost: 0.9140758646107909\n",
            "EPOCH: 112\n",
            "Cost: 0.9116576241398435\n",
            "EPOCH: 113\n",
            "Cost: 0.9056374020760714\n",
            "EPOCH: 114\n",
            "Cost: 0.9037789894303117\n",
            "EPOCH: 115\n",
            "Cost: 0.8979381579957113\n",
            "EPOCH: 116\n",
            "Cost: 0.8946437036008421\n",
            "EPOCH: 117\n",
            "Cost: 0.8917169074906819\n",
            "EPOCH: 118\n",
            "Cost: 0.8876481996237295\n",
            "EPOCH: 119\n",
            "Cost: 0.8844067708941784\n",
            "EPOCH: 120\n",
            "Cost: 0.8809013856985408\n",
            "EPOCH: 121\n",
            "Cost: 0.8755605759583929\n",
            "EPOCH: 122\n",
            "Cost: 0.8727616338103743\n",
            "EPOCH: 123\n",
            "Cost: 0.8672125762597022\n",
            "EPOCH: 124\n",
            "Cost: 0.8668336871394304\n",
            "EPOCH: 125\n",
            "Cost: 0.8606077234800481\n",
            "EPOCH: 126\n",
            "Cost: 0.8585851304869705\n",
            "EPOCH: 127\n",
            "Cost: 0.8550842230933318\n",
            "EPOCH: 128\n",
            "Cost: 0.853266994893356\n",
            "EPOCH: 129\n",
            "Cost: 0.8493140239385853\n",
            "EPOCH: 130\n",
            "Cost: 0.8449044842563336\n",
            "EPOCH: 131\n",
            "Cost: 0.8391188148449849\n",
            "EPOCH: 132\n",
            "Cost: 0.8376717648195069\n",
            "EPOCH: 133\n",
            "Cost: 0.8341476851351668\n",
            "EPOCH: 134\n",
            "Cost: 0.8311308863893033\n",
            "EPOCH: 135\n",
            "Cost: 0.8296738658597775\n",
            "EPOCH: 136\n",
            "Cost: 0.8247278628838658\n",
            "EPOCH: 137\n",
            "Cost: 0.8223574752341334\n",
            "EPOCH: 138\n",
            "Cost: 0.8198676782559897\n",
            "EPOCH: 139\n",
            "Cost: 0.8156986767679346\n",
            "EPOCH: 140\n",
            "Cost: 0.8139889622503715\n",
            "EPOCH: 141\n",
            "Cost: 0.8111996422282841\n",
            "EPOCH: 142\n",
            "Cost: 0.8084859320038768\n",
            "EPOCH: 143\n",
            "Cost: 0.8052044088009429\n",
            "EPOCH: 144\n",
            "Cost: 0.8041776451476517\n",
            "EPOCH: 145\n",
            "Cost: 0.7994246080885231\n",
            "EPOCH: 146\n",
            "Cost: 0.7957362959083045\n",
            "EPOCH: 147\n",
            "Cost: 0.7942839715079867\n",
            "EPOCH: 148\n",
            "Cost: 0.7918513242675358\n",
            "EPOCH: 149\n",
            "Cost: 0.7883782957617198\n",
            "EPOCH: 150\n",
            "Cost: 0.7858966908382852\n",
            "EPOCH: 151\n",
            "Cost: 0.7812679702051397\n",
            "EPOCH: 152\n",
            "Cost: 0.7800269753648017\n",
            "EPOCH: 153\n",
            "Cost: 0.7771637638659216\n",
            "EPOCH: 154\n",
            "Cost: 0.7736490907432069\n",
            "EPOCH: 155\n",
            "Cost: 0.7718971922772859\n",
            "EPOCH: 156\n",
            "Cost: 0.7690120037449918\n",
            "EPOCH: 157\n",
            "Cost: 0.7660767876659429\n",
            "EPOCH: 158\n",
            "Cost: 0.762713972583883\n",
            "EPOCH: 159\n",
            "Cost: 0.7629940987479293\n",
            "EPOCH: 160\n",
            "Cost: 0.761482577938235\n",
            "EPOCH: 161\n",
            "Cost: 0.758230414091022\n",
            "EPOCH: 162\n",
            "Cost: 0.7557823565151071\n",
            "EPOCH: 163\n",
            "Cost: 0.7535664492872625\n",
            "EPOCH: 164\n",
            "Cost: 0.7506890308847617\n",
            "EPOCH: 165\n",
            "Cost: 0.748891821305377\n",
            "EPOCH: 166\n",
            "Cost: 0.7457011341767703\n",
            "EPOCH: 167\n",
            "Cost: 0.7442360373957562\n",
            "EPOCH: 168\n",
            "Cost: 0.7416997645786221\n",
            "EPOCH: 169\n",
            "Cost: 0.7386827545049932\n",
            "EPOCH: 170\n",
            "Cost: 0.7357315068059154\n",
            "EPOCH: 171\n",
            "Cost: 0.7325644277103336\n",
            "EPOCH: 172\n",
            "Cost: 0.7311002722954023\n",
            "EPOCH: 173\n",
            "Cost: 0.730752108201953\n",
            "EPOCH: 174\n",
            "Cost: 0.7270695439431039\n",
            "EPOCH: 175\n",
            "Cost: 0.7263854927075601\n",
            "EPOCH: 176\n",
            "Cost: 0.7246378062706957\n",
            "EPOCH: 177\n",
            "Cost: 0.7205623907733136\n",
            "EPOCH: 178\n",
            "Cost: 0.7206441457573337\n",
            "EPOCH: 179\n",
            "Cost: 0.7171654154375542\n",
            "EPOCH: 180\n",
            "Cost: 0.7140499565304551\n",
            "EPOCH: 181\n",
            "Cost: 0.7143494160796889\n",
            "EPOCH: 182\n",
            "Cost: 0.7116193956817061\n",
            "EPOCH: 183\n",
            "Cost: 0.7110579241485744\n",
            "EPOCH: 184\n",
            "Cost: 0.7084336705216552\n",
            "EPOCH: 185\n",
            "Cost: 0.7065947501387824\n",
            "EPOCH: 186\n",
            "Cost: 0.7031836597877701\n",
            "EPOCH: 187\n",
            "Cost: 0.7012292166605433\n",
            "EPOCH: 188\n",
            "Cost: 0.7016013858125143\n",
            "EPOCH: 189\n",
            "Cost: 0.6985319843854765\n",
            "EPOCH: 190\n",
            "Cost: 0.6976827311925946\n",
            "EPOCH: 191\n",
            "Cost: 0.6963476589971604\n",
            "EPOCH: 192\n",
            "Cost: 0.6919947575750323\n",
            "EPOCH: 193\n",
            "Cost: 0.69190778829557\n",
            "EPOCH: 194\n",
            "Cost: 0.6894186345150937\n",
            "EPOCH: 195\n",
            "Cost: 0.6847799342398102\n",
            "EPOCH: 196\n",
            "Cost: 0.6857251012557832\n",
            "EPOCH: 197\n",
            "Cost: 0.6828004718356084\n",
            "EPOCH: 198\n",
            "Cost: 0.68376379998443\n",
            "EPOCH: 199\n",
            "Cost: 0.6786350200023796\n",
            "EPOCH: 200\n",
            "Cost: 0.6773825037088508\n",
            "EPOCH: 201\n",
            "Cost: 0.6785489812831396\n",
            "EPOCH: 202\n",
            "Cost: 0.6756052128505663\n",
            "EPOCH: 203\n",
            "Cost: 0.674264611356914\n",
            "EPOCH: 204\n",
            "Cost: 0.6716381101228962\n",
            "EPOCH: 205\n",
            "Cost: 0.6701741210428136\n",
            "EPOCH: 206\n",
            "Cost: 0.6675895168589812\n",
            "EPOCH: 207\n",
            "Cost: 0.6674693740504453\n",
            "EPOCH: 208\n",
            "Cost: 0.6686756599613688\n",
            "EPOCH: 209\n",
            "Cost: 0.6665180182164084\n",
            "EPOCH: 210\n",
            "Cost: 0.6633610295791923\n",
            "EPOCH: 211\n",
            "Cost: 0.6614157782665847\n",
            "EPOCH: 212\n",
            "Cost: 0.6596022381563387\n",
            "EPOCH: 213\n",
            "Cost: 0.6579610052663578\n",
            "EPOCH: 214\n",
            "Cost: 0.6578965010456639\n",
            "EPOCH: 215\n",
            "Cost: 0.6558473873378673\n",
            "EPOCH: 216\n",
            "Cost: 0.654795189598926\n",
            "EPOCH: 217\n",
            "Cost: 0.6526272768612311\n",
            "EPOCH: 218\n",
            "Cost: 0.6525177919657219\n",
            "EPOCH: 219\n",
            "Cost: 0.6490749702802727\n",
            "EPOCH: 220\n",
            "Cost: 0.6495348726175709\n",
            "EPOCH: 221\n",
            "Cost: 0.6477898867704346\n",
            "EPOCH: 222\n",
            "Cost: 0.6464787659958594\n",
            "EPOCH: 223\n",
            "Cost: 0.6463364780038598\n",
            "EPOCH: 224\n",
            "Cost: 0.6440157270151771\n",
            "EPOCH: 225\n",
            "Cost: 0.6416064065357019\n",
            "EPOCH: 226\n",
            "Cost: 0.6414641763276032\n",
            "EPOCH: 227\n",
            "Cost: 0.6387256834127961\n",
            "EPOCH: 228\n",
            "Cost: 0.6371481348204064\n",
            "EPOCH: 229\n",
            "Cost: 0.6372923123839331\n",
            "EPOCH: 230\n",
            "Cost: 0.6367338323124297\n",
            "EPOCH: 231\n",
            "Cost: 0.6338124697196137\n",
            "EPOCH: 232\n",
            "Cost: 0.6338756380329735\n",
            "EPOCH: 233\n",
            "Cost: 0.6308719864119225\n",
            "EPOCH: 234\n",
            "Cost: 0.6312080239877917\n",
            "EPOCH: 235\n",
            "Cost: 0.6307835261125003\n",
            "EPOCH: 236\n",
            "Cost: 0.6280729049515431\n",
            "EPOCH: 237\n",
            "Cost: 0.6270564364970694\n",
            "EPOCH: 238\n",
            "Cost: 0.6252008825647227\n",
            "EPOCH: 239\n",
            "Cost: 0.6232207817902322\n",
            "EPOCH: 240\n",
            "Cost: 0.6254816530232421\n",
            "EPOCH: 241\n",
            "Cost: 0.6214964680410435\n",
            "EPOCH: 242\n",
            "Cost: 0.6233395143485438\n",
            "EPOCH: 243\n",
            "Cost: 0.6211643410806297\n",
            "EPOCH: 244\n",
            "Cost: 0.6193939013428813\n",
            "EPOCH: 245\n",
            "Cost: 0.6188840818318397\n",
            "EPOCH: 246\n",
            "Cost: 0.6162104491145958\n",
            "EPOCH: 247\n",
            "Cost: 0.614780790970566\n",
            "EPOCH: 248\n",
            "Cost: 0.6162726447751388\n",
            "EPOCH: 249\n",
            "Cost: 0.6133706746948825\n",
            "EPOCH: 250\n",
            "Cost: 0.612892192486437\n",
            "EPOCH: 251\n",
            "Cost: 0.6110120783505377\n",
            "EPOCH: 252\n",
            "Cost: 0.6092410375517492\n",
            "EPOCH: 253\n",
            "Cost: 0.6103508248595404\n",
            "EPOCH: 254\n",
            "Cost: 0.6097960630635426\n",
            "EPOCH: 255\n",
            "Cost: 0.6069281034680745\n",
            "EPOCH: 256\n",
            "Cost: 0.6052409834699297\n",
            "EPOCH: 257\n",
            "Cost: 0.6037356510544092\n",
            "EPOCH: 258\n",
            "Cost: 0.605362416856061\n",
            "EPOCH: 259\n",
            "Cost: 0.603919970994784\n",
            "EPOCH: 260\n",
            "Cost: 0.6029515258040978\n",
            "EPOCH: 261\n",
            "Cost: 0.601482427220337\n",
            "EPOCH: 262\n",
            "Cost: 0.600807604174578\n",
            "EPOCH: 263\n",
            "Cost: 0.6002055555747413\n",
            "EPOCH: 264\n",
            "Cost: 0.5993166164196833\n",
            "EPOCH: 265\n",
            "Cost: 0.5965788718922393\n",
            "EPOCH: 266\n",
            "Cost: 0.5960128127482769\n",
            "EPOCH: 267\n",
            "Cost: 0.5960377752699948\n",
            "EPOCH: 268\n",
            "Cost: 0.5951186366782849\n",
            "EPOCH: 269\n",
            "Cost: 0.5938701832052773\n",
            "EPOCH: 270\n",
            "Cost: 0.5933489524692295\n",
            "EPOCH: 271\n",
            "Cost: 0.5924686610287052\n",
            "EPOCH: 272\n",
            "Cost: 0.5911182723543131\n",
            "EPOCH: 273\n",
            "Cost: 0.5929502864193403\n",
            "EPOCH: 274\n",
            "Cost: 0.5904052781905447\n",
            "EPOCH: 275\n",
            "Cost: 0.5889613474844448\n",
            "EPOCH: 276\n",
            "Cost: 0.5884300477376749\n",
            "EPOCH: 277\n",
            "Cost: 0.5888142671932968\n",
            "EPOCH: 278\n",
            "Cost: 0.5874584288877412\n",
            "EPOCH: 279\n",
            "Cost: 0.586005746156355\n",
            "EPOCH: 280\n",
            "Cost: 0.5845019764985083\n",
            "EPOCH: 281\n",
            "Cost: 0.5821415046875633\n",
            "EPOCH: 282\n",
            "Cost: 0.585125843600516\n",
            "EPOCH: 283\n",
            "Cost: 0.5830203819705391\n",
            "EPOCH: 284\n",
            "Cost: 0.5830546598759173\n",
            "EPOCH: 285\n",
            "Cost: 0.5810917285937707\n",
            "EPOCH: 286\n",
            "Cost: 0.5798733956659103\n",
            "EPOCH: 287\n",
            "Cost: 0.5798321384058197\n",
            "EPOCH: 288\n",
            "Cost: 0.5773294342149585\n",
            "EPOCH: 289\n",
            "Cost: 0.5794976013942201\n",
            "EPOCH: 290\n",
            "Cost: 0.5769699613540877\n",
            "EPOCH: 291\n",
            "Cost: 0.5746713851705691\n",
            "EPOCH: 292\n",
            "Cost: 0.5744035921520567\n",
            "EPOCH: 293\n",
            "Cost: 0.5764724808709135\n",
            "EPOCH: 294\n",
            "Cost: 0.5751553189762741\n",
            "EPOCH: 295\n",
            "Cost: 0.5732105877767273\n",
            "EPOCH: 296\n",
            "Cost: 0.5744333292908917\n",
            "EPOCH: 297\n",
            "Cost: 0.570694300894925\n",
            "EPOCH: 298\n",
            "Cost: 0.5713134605881319\n",
            "EPOCH: 299\n",
            "Cost: 0.5711056930744185\n",
            "EPOCH: 300\n",
            "Cost: 0.5701568310347135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ahLfF2UVFVpO",
        "outputId": "baf8d0d3-b338-47c2-e73d-29528491cf0b"
      },
      "source": [
        "# Displaying the history\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "x = [(i + 1) for i in range(len(history))]\r\n",
        "\r\n",
        "plt.plot(x, history)\r\n",
        "plt.title(\"Neural network trained on MNIST: Epochs vs Loss\")\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"MSE Loss\");\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}