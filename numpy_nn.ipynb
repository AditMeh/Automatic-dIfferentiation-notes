{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_graphs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNW5r52zeff6DKQ/uuVBxtM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AditMeh/deep-learning-from-scratch/blob/main/numpy_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR-wmCl4L-_M"
      },
      "source": [
        "\n",
        "\n",
        "#### Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXJ1MVcLLy5u"
      },
      "source": [
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi03FGJBMN_W"
      },
      "source": [
        "**Sources:**\n",
        "- https://stats.stackexchange.com/questions/224140/step-by-step-example-of-reverse-mode-automatic-differentiation\n",
        "\n",
        "- https://www.youtube.com/watch?v=EEbnprb_YTU&ab_channel=NathanSprague\n",
        "\n",
        "- https://www.youtube.com/watch?v=twTIGuVhKbQ&ab_channel=JorisGillis\n",
        "\n",
        "- http://neuralnetworksanddeeplearning.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u79VCuDRnn8P"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNet:\n",
        "    def __init__(self, layer_list):\n",
        "        self.layers = len(layer_list)\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "\n",
        "        #randomly initialize weights and biases (scale each random value by 0.01 to prevent vanishing gradient due the saturation of sigmoid)\n",
        "        for layer_index in range(1, len(layer_list)):\n",
        "            self.weights.append(np.random.rand(layer_list[layer_index], layer_list[layer_index - 1])*0.01)\n",
        "            self.biases.append(np.random.rand(layer_list[layer_index], 1)*0.01)\n",
        "        \n",
        "        print(\"Weights\")\n",
        "        print([element.shape for element in self.weights])\n",
        "\n",
        "        print(\"\\n\" + \"Biases\")\n",
        "        print([element.shape for element in self.biases])\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1/(1 + (np.e)**(-x))\n",
        "    def sigmoid_prime(self, x):\n",
        "        return self.sigmoid(x)*(1 - self.sigmoid(x))\n",
        "    def compute_cost(self, real, predicted):\n",
        "        return np.sum(1/2*(predicted - real)**2)\n",
        "    def cost_derivative(self, real, predicted):\n",
        "        return (predicted - real)\n",
        "    \n",
        "    def feedforward(self, x, output):\n",
        "       self.x = x\n",
        "       self.output = output\n",
        "\n",
        "       #Activation of the input layer is equivalent to the inputs that are passed in\n",
        "       current_a = self.x\n",
        "\n",
        "       self.z = []\n",
        "       self.a = [self.x]\n",
        "       for w_i, b_i in zip(self.weights, self.biases):\n",
        "           z_i = np.dot(w_i, current_a) + b_i\n",
        "           self.z.append(z_i)\n",
        "           current_a = self.sigmoid(z_i)\n",
        "           self.a.append(current_a)\n",
        "\n",
        "       return self.compute_cost(output, current_a)\n",
        "        \n",
        "    def compute_gradients_backprop(self):\n",
        "        w_i_grad = [np.zeros(element.shape) for element in self.weights]\n",
        "        b_i_grad = [np.zeros(element.shape) for element in self.biases]\n",
        "\n",
        "\n",
        "        #compute final layer error\n",
        "        delta  = self.cost_derivative(self.output, (self.a)[-1]) * self.sigmoid_prime((self.z)[-1])\n",
        "        \n",
        "        #compute gradients for weight in layer L-1\n",
        "        dCdW_final_layer = np.dot(delta, self.a[-2].T)\n",
        "        \n",
        "        w_i_grad[-1] = dCdW_final_layer\n",
        "        b_i_grad[-1] = delta\n",
        "\n",
        "        for i in range(2, self.layers):\n",
        "            z_current = self.z[-i]\n",
        "\n",
        "            #computing delta for layer l\n",
        "            delta = np.dot(self.weights[-i + 1].T, delta) * self.sigmoid_prime(z_current)\n",
        "\n",
        "            #computing dC/dW for layer l\n",
        "            dW = np.dot(delta, self.a[-i - 1].T)\n",
        "\n",
        "            w_i_grad[-i] = dW\n",
        "            b_i_grad[-i] = delta\n",
        "        return w_i_grad, b_i_grad\n",
        "\n",
        "    def update_weights_and_biases(self, w_grad, b_grad, learning_rate):        \n",
        "\n",
        "        #apply gradients to current weights\n",
        "        for i in range(len(self.weights)): \n",
        "            self.weights[i] -= learning_rate*w_grad[i]\n",
        "            self.biases[i] -= learning_rate*b_grad[i]\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9P_bc193FUT"
      },
      "source": [
        "#Testing on MNIST using SGD\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2]) / 255\n",
        "\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n",
        "\n",
        "\n",
        "\n",
        "def prepare_mini_batches(minibatch_size: int):\n",
        "    \"\"\"\n",
        "    Takes in a minibatch size parameter and generates random indexes from the\n",
        "    training set (X_train). Returns a list of indexes of length minibatch_size.\n",
        "    \"\"\"\n",
        "    random_indexes = np.random.choice(X_train.shape[0], minibatch_size, replace=False)\n",
        "    return random_indexes\n",
        "\n",
        "def one_hot_encode(length: int, index: int):\n",
        "    \"\"\"\n",
        "    Takes in a length and index parameter to one hot encode a label. Returns\n",
        "    a one hot encoded numpy array of shape (# of classes, 1)\n",
        "    \"\"\"\n",
        "    output = [0 for i in range(length)]\n",
        "    output[index] = 1\n",
        "    return np.asarray(output).reshape(length, 1)\n",
        "\n",
        "\n",
        "def compute_test_loss(net: NeuralNet):\n",
        "    \"\"\"\n",
        "    Computes the average test loss of the neural network on my X_test numpy \n",
        "    array. \n",
        "    \"\"\"\n",
        "    total_loss = []\n",
        "    for i in range(len(X_test)):\n",
        "        loss = net.feedforward(X_test[i].reshape(X_test[i].shape[0], 1), one_hot_encode(10 ,Y_test[i]))\n",
        "        total_loss.append(loss)\n",
        "    return sum(total_loss)/len(total_loss)\n",
        "\n",
        "#instantiate neural net\n",
        "nn = NeuralNet([784, 392, 196, 10])\n",
        "\n",
        "\n",
        "#training loop\n",
        "epochs = 300\n",
        "mini_size = 32\n",
        "\n",
        "for i in range(epochs):\n",
        "    \n",
        "    print(\"EPOCH: \" + str(i + 1))\n",
        "\n",
        "    #Generates a randomly chosen set of training indexes per epoch\n",
        "    random_indexes = prepare_mini_batches(mini_size)\n",
        "        \n",
        "    #placeholders for gradient of weights and biases for the neural network\n",
        "    grads_w = [np.zeros(element.shape) for element in nn.weights]\n",
        "    grads_b = [np.zeros(element.shape) for element in nn.biases]\n",
        "\n",
        "    #cost accumulator for each observation in the minibatch \n",
        "    cost_accumulator = []\n",
        "\n",
        "    #iterating through each index in the current minibatch o\n",
        "    for train_idx in minibatch:\n",
        "        cost_iter = nn.feedforward(X_train[train_idx].reshape(X_train[train_idx].shape[0], 1), one_hot_encode(10 ,Y_train[train_idx]))\n",
        "        cost_accumulator.append(cost_iter)\n",
        "        grad_w, grad_b = nn.compute_gradients_backprop()\n",
        "\n",
        "        for i in range(len(grad_w)):\n",
        "            grads_w[i] += grad_w[i]\n",
        "            grads_b[i] += grad_b[i]\n",
        "\n",
        "    grads_w = [grads_w[i]/mini_size for i in range(len(grad_w))]\n",
        "    grads_b = [grads_b[i]/mini_size for i in range(len(grad_b))]\n",
        "\n",
        "    print(\"Cost for batch  = \" + str(sum(cost)/len(cost)))\n",
        "        \n",
        "    #update weights\n",
        "    nn.update_weights_and_biases(grads_w, grads_b, 0.01)\n",
        "\n",
        "    #computing test loss\n",
        "    print(\"test loss = \" + str(compute_test_loss(nn)) + \"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}