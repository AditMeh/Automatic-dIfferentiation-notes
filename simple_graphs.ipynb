{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_graphs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN/39Z1SHf8oR+O7w+n8Wny",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AditMeh/Automatic-differentiation-notes/blob/main/simple_graphs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR-wmCl4L-_M"
      },
      "source": [
        "\n",
        "\n",
        "#### Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXJ1MVcLLy5u"
      },
      "source": [
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi03FGJBMN_W"
      },
      "source": [
        "**Sources:**\n",
        "- https://stats.stackexchange.com/questions/224140/step-by-step-example-of-reverse-mode-automatic-differentiation\n",
        "\n",
        "- https://www.youtube.com/watch?v=EEbnprb_YTU&ab_channel=NathanSprague\n",
        "\n",
        "- https://www.youtube.com/watch?v=twTIGuVhKbQ&ab_channel=JorisGillis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u79VCuDRnn8P"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNet:\n",
        "    def __init__(self, layer_list):\n",
        "        self.layers = len(layer_list)\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        for layer_index in range(1, len(layer_list)):\n",
        "            self.weights.append(np.random.rand(layer_list[layer_index], layer_list[layer_index - 1])*0.01)\n",
        "            self.biases.append(np.random.rand(layer_list[layer_index], 1)*0.01)\n",
        "        \n",
        "        print(\"Weights\")\n",
        "        print([element.shape for element in self.weights])\n",
        "\n",
        "        print(\"\\n\" + \"Biases\")\n",
        "        print([element.shape for element in self.biases])\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1/(1 + (np.e)**(-x))\n",
        "    def sigmoid_prime(self, x):\n",
        "        return self.sigmoid(x)*(1 - self.sigmoid(x))\n",
        "    def compute_cost(self, real, predicted):\n",
        "        return np.sum(1/2*(predicted - real)**2)\n",
        "    def cost_derivative(self, real, predicted):\n",
        "        return (predicted - real)\n",
        "    \n",
        "    def feedforward(self, x, output):\n",
        "       self.x = x\n",
        "       current_a = self.x\n",
        "       self.z = []\n",
        "       self.a = [self.x]\n",
        "       for w_i, b_i in zip(self.weights, self.biases):\n",
        "           z_i = np.dot(w_i, current_a) + b_i\n",
        "           self.z.append(z_i)\n",
        "           current_a = self.sigmoid(z_i)\n",
        "           self.a.append(current_a)\n",
        "\n",
        "       return self.compute_cost(output, current_a)\n",
        "        \n",
        "    def compute_gradients_backprop(self, output):\n",
        "        w_i_grad = [np.zeros(element.shape) for element in self.weights]\n",
        "        b_i_grad = [np.zeros(element.shape) for element in self.biases]\n",
        "\n",
        "\n",
        "        #compute final layer error\n",
        "        delta  = self.cost_derivative(output, (self.a)[-1]) * self.sigmoid_prime((self.z)[-1])\n",
        "        \n",
        "        \n",
        "        dCdW_final_layer = np.dot(delta, self.a[-2].T)\n",
        "        \n",
        "        w_i_grad[-1] = dCdW_final_layer\n",
        "        b_i_grad[-1] = delta\n",
        "\n",
        "        for i in range(2, self.layers):\n",
        "            z_current = self.z[-i]\n",
        "\n",
        "\n",
        "            delta = np.dot(self.weights[-i + 1].T, delta) * self.sigmoid_prime(z_current)\n",
        "\n",
        "            dW = np.dot(delta, self.a[-i - 1].T)\n",
        "\n",
        "            w_i_grad[-i] = dW\n",
        "            b_i_grad[-i] = delta\n",
        "        return w_i_grad, b_i_grad\n",
        "\n",
        "    def update_weights_and_biases(self, w_grad, b_grad, learning_rate):        \n",
        "\n",
        "        #apply gradients\n",
        "        for i in range(len(self.weights)): \n",
        "            self.weights[i] -= learning_rate*w_grad[i]\n",
        "            self.biases[i] -= learning_rate*b_grad[i]\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9P_bc193FUT",
        "outputId": "849c0264-40a5-4eca-dd8c-1453cd317cb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Testing on MNIST using SGD\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2]) / 255\n",
        "\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n",
        "\n",
        "def prepare_mini_batches(minibatch_size):\n",
        "    random_indexes = []\n",
        "    indexes = np.random.choice(X_train.shape[0], minibatch_size, replace=False)\n",
        "    random_indexes.append(indexes) \n",
        "    return random_indexes\n",
        "\n",
        "def one_hot_encode(length, index):\n",
        "    output = [0 for i in range(length)]\n",
        "    output[index] = 1\n",
        "    return np.asarray(output).reshape(length, 1)\n",
        "\n",
        "\n",
        "def compute_test_loss(test_set, net):\n",
        "    total_loss = []\n",
        "    for i in range(len(X_test)):\n",
        "        loss = net.feedforward(X_test[i].reshape(X_test[i].shape[0], 1), one_hot_encode(10 ,Y_test[i]))\n",
        "        total_loss.append(loss)\n",
        "    return sum(total_loss)/len(total_loss)\n",
        "\n",
        "#instantiate neural net\n",
        "nn = NeuralNet([784, 392, 196, 10])\n",
        "\n",
        "\n",
        "#training loop\n",
        "epochs = 300\n",
        "mini_size = 32\n",
        "for i in range(epochs):\n",
        "    random_indexes = prepare_mini_batches(mini_size)\n",
        "\n",
        "    for minibatch in random_indexes:\n",
        "        grads_w = [np.zeros(element.shape) for element in nn.weights]\n",
        "        grads_b = [np.zeros(element.shape) for element in nn.biases]\n",
        "        cost = []\n",
        "        for j in minibatch:\n",
        "            cost_iter = nn.feedforward(X_train[j].reshape(X_train[j].shape[0], 1), one_hot_encode(10 ,Y_train[j]))\n",
        "            cost.append(cost_iter)\n",
        "            grad_w, grad_b = nn.compute_gradients_backprop(one_hot_encode(10, Y_train[j]))\n",
        "\n",
        "            for i in range(len(grad_w)):\n",
        "                grads_w[i] += grad_w[i]\n",
        "                grads_b[i] += grad_b[i]\n",
        "        grads_w = [grads_w[i]/mini_size for i in range(len(grad_w))]\n",
        "        grads_b = [grads_b[i]/mini_size for i in range(len(grad_b))]\n",
        "\n",
        "        #update weights\n",
        "        print(\"Cost for batch  = \" + str(sum(cost)/len(cost)))\n",
        "        print(\"updating weights\")\n",
        "        nn.update_weights_and_biases(grads_w, grads_b, 0.01)\n",
        "        print(\"test loss = \" + str(compute_test_loss(X_test, nn)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights\n",
            "[(392, 784), (196, 392), (10, 196)]\n",
            "\n",
            "Biases\n",
            "[(392, 1), (196, 1), (10, 1)]\n",
            "Cost for batch  = 2.1398411560725386\n",
            "updating weights\n",
            "test loss = 2.058023376824719\n",
            "Cost for batch  = 1.9533218804125139\n",
            "updating weights\n",
            "test loss = 1.8496265713740996\n",
            "Cost for batch  = 1.7755249384113183\n",
            "updating weights\n",
            "test loss = 1.6518069431240399\n",
            "Cost for batch  = 1.6041963240614319\n",
            "updating weights\n",
            "test loss = 1.4706860062097227\n",
            "Cost for batch  = 1.442206608590869\n",
            "updating weights\n",
            "test loss = 1.3121096299662305\n",
            "Cost for batch  = 1.3061595424652779\n",
            "updating weights\n",
            "test loss = 1.1750746014358078\n",
            "Cost for batch  = 1.1836407393363197\n",
            "updating weights\n",
            "test loss = 1.059658966851878\n",
            "Cost for batch  = 1.0794004723895567\n",
            "updating weights\n",
            "test loss = 0.9629488865477049\n",
            "Cost for batch  = 0.9952508868351942\n",
            "updating weights\n",
            "test loss = 0.8832232439647836\n",
            "Cost for batch  = 0.9193169965605776\n",
            "updating weights\n",
            "test loss = 0.8174836380757027\n",
            "Cost for batch  = 0.854990241267479\n",
            "updating weights\n",
            "test loss = 0.7633630914666975\n",
            "Cost for batch  = 0.8061514606313281\n",
            "updating weights\n",
            "test loss = 0.7187686528462083\n",
            "Cost for batch  = 0.7637420155330142\n",
            "updating weights\n",
            "test loss = 0.6816220776319593\n",
            "Cost for batch  = 0.7271381851051895\n",
            "updating weights\n",
            "test loss = 0.6505636399434981\n",
            "Cost for batch  = 0.695966573728516\n",
            "updating weights\n",
            "test loss = 0.6245587675580678\n",
            "Cost for batch  = 0.6673322136933892\n",
            "updating weights\n",
            "test loss = 0.6027501074896965\n",
            "Cost for batch  = 0.6473134528360985\n",
            "updating weights\n",
            "test loss = 0.5842572317018324\n",
            "Cost for batch  = 0.6248526310200117\n",
            "updating weights\n",
            "test loss = 0.5686174681014904\n",
            "Cost for batch  = 0.607830620481719\n",
            "updating weights\n",
            "test loss = 0.5552538862557551\n",
            "Cost for batch  = 0.5964739955086565\n",
            "updating weights\n",
            "test loss = 0.543601374604189\n",
            "Cost for batch  = 0.5813850384723681\n",
            "updating weights\n",
            "test loss = 0.5335989494543413\n",
            "Cost for batch  = 0.5713808029869417\n",
            "updating weights\n",
            "test loss = 0.5249806775224191\n",
            "Cost for batch  = 0.5598958653740783\n",
            "updating weights\n",
            "test loss = 0.5173288464152342\n",
            "Cost for batch  = 0.5511050386161451\n",
            "updating weights\n",
            "test loss = 0.5106333411285005\n",
            "Cost for batch  = 0.5441442927625106\n",
            "updating weights\n",
            "test loss = 0.5047427523210406\n",
            "Cost for batch  = 0.5356090323062906\n",
            "updating weights\n",
            "test loss = 0.4996362536496681\n",
            "Cost for batch  = 0.5305478751060846\n",
            "updating weights\n",
            "test loss = 0.49504214112132006\n",
            "Cost for batch  = 0.5272327943072717\n",
            "updating weights\n",
            "test loss = 0.4908352117081549\n",
            "Cost for batch  = 0.5164057863076013\n",
            "updating weights\n",
            "test loss = 0.4873752463402224\n",
            "Cost for batch  = 0.5137270766337558\n",
            "updating weights\n",
            "test loss = 0.4841663433569138\n",
            "Cost for batch  = 0.5111767947297942\n",
            "updating weights\n",
            "test loss = 0.48117371900841255\n",
            "Cost for batch  = 0.5088738941415071\n",
            "updating weights\n",
            "test loss = 0.47839551404736713\n",
            "Cost for batch  = 0.5014609081761903\n",
            "updating weights\n",
            "test loss = 0.4761162873429381\n",
            "Cost for batch  = 0.4992439242858692\n",
            "updating weights\n",
            "test loss = 0.4739439283101731\n",
            "Cost for batch  = 0.4975683622232735\n",
            "updating weights\n",
            "test loss = 0.47194315967560296\n",
            "Cost for batch  = 0.49293883082186507\n",
            "updating weights\n",
            "test loss = 0.4702359933854568\n",
            "Cost for batch  = 0.49126053363170175\n",
            "updating weights\n",
            "test loss = 0.4686052129882736\n",
            "Cost for batch  = 0.4883901228849279\n",
            "updating weights\n",
            "test loss = 0.46716732463979543\n",
            "Cost for batch  = 0.49035819355392546\n",
            "updating weights\n",
            "test loss = 0.4657676266106261\n",
            "Cost for batch  = 0.48391155368585065\n",
            "updating weights\n",
            "test loss = 0.4646320101755216\n",
            "Cost for batch  = 0.4829024684241385\n",
            "updating weights\n",
            "test loss = 0.4635037143516626\n",
            "Cost for batch  = 0.48173505273279593\n",
            "updating weights\n",
            "test loss = 0.4624630155020487\n",
            "Cost for batch  = 0.48119777033226263\n",
            "updating weights\n",
            "test loss = 0.46147987244405386\n",
            "Cost for batch  = 0.4786060813539197\n",
            "updating weights\n",
            "test loss = 0.4606066512718842\n",
            "Cost for batch  = 0.4770954347630839\n",
            "updating weights\n",
            "test loss = 0.45977800987600786\n",
            "Cost for batch  = 0.47360563558255364\n",
            "updating weights\n",
            "test loss = 0.4591212106056463\n",
            "Cost for batch  = 0.4755246574667535\n",
            "updating weights\n",
            "test loss = 0.4584372911898621\n",
            "Cost for batch  = 0.4722357150345712\n",
            "updating weights\n",
            "test loss = 0.45789996765175156\n",
            "Cost for batch  = 0.47312626195916513\n",
            "updating weights\n",
            "test loss = 0.4572745010114518\n",
            "Cost for batch  = 0.47143891497843093\n",
            "updating weights\n",
            "test loss = 0.45671652872976165\n",
            "Cost for batch  = 0.47048674963073284\n",
            "updating weights\n",
            "test loss = 0.45619480291714765\n",
            "Cost for batch  = 0.4682871286773562\n",
            "updating weights\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-957d1da57a5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"updating weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights_and_biases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test loss = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_test_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-957d1da57a5e>\u001b[0m in \u001b[0;36mcompute_test_loss\u001b[0;34m(test_set, net)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-9e28673d1bc4>\u001b[0m in \u001b[0;36mfeedforward\u001b[0;34m(self, x, output)\u001b[0m\n\u001b[1;32m     32\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mw_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m            \u001b[0mz_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_a\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m            \u001b[0mcurrent_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}