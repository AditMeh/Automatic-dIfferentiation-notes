{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_graphs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN/39Z1SHf8oR+O7w+n8Wny",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AditMeh/deep-learning-from-scratch/blob/main/simple_graphs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR-wmCl4L-_M"
      },
      "source": [
        "\n",
        "\n",
        "#### Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXJ1MVcLLy5u"
      },
      "source": [
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi03FGJBMN_W"
      },
      "source": [
        "**Sources:**\n",
        "- https://stats.stackexchange.com/questions/224140/step-by-step-example-of-reverse-mode-automatic-differentiation\n",
        "\n",
        "- https://www.youtube.com/watch?v=EEbnprb_YTU&ab_channel=NathanSprague\n",
        "\n",
        "- https://www.youtube.com/watch?v=twTIGuVhKbQ&ab_channel=JorisGillis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u79VCuDRnn8P"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNet:\n",
        "    def __init__(self, layer_list):\n",
        "        self.layers = len(layer_list)\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        for layer_index in range(1, len(layer_list)):\n",
        "            self.weights.append(np.random.rand(layer_list[layer_index], layer_list[layer_index - 1])*0.01)\n",
        "            self.biases.append(np.random.rand(layer_list[layer_index], 1)*0.01)\n",
        "        \n",
        "        print(\"Weights\")\n",
        "        print([element.shape for element in self.weights])\n",
        "\n",
        "        print(\"\\n\" + \"Biases\")\n",
        "        print([element.shape for element in self.biases])\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1/(1 + (np.e)**(-x))\n",
        "    def sigmoid_prime(self, x):\n",
        "        return self.sigmoid(x)*(1 - self.sigmoid(x))\n",
        "    def compute_cost(self, real, predicted):\n",
        "        return np.sum(1/2*(predicted - real)**2)\n",
        "    def cost_derivative(self, real, predicted):\n",
        "        return (predicted - real)\n",
        "    \n",
        "    def feedforward(self, x, output):\n",
        "       self.x = x\n",
        "       current_a = self.x\n",
        "       self.z = []\n",
        "       self.a = [self.x]\n",
        "       for w_i, b_i in zip(self.weights, self.biases):\n",
        "           z_i = np.dot(w_i, current_a) + b_i\n",
        "           self.z.append(z_i)\n",
        "           current_a = self.sigmoid(z_i)\n",
        "           self.a.append(current_a)\n",
        "\n",
        "       return self.compute_cost(output, current_a)\n",
        "        \n",
        "    def compute_gradients_backprop(self, output):\n",
        "        w_i_grad = [np.zeros(element.shape) for element in self.weights]\n",
        "        b_i_grad = [np.zeros(element.shape) for element in self.biases]\n",
        "\n",
        "\n",
        "        #compute final layer error\n",
        "        delta  = self.cost_derivative(output, (self.a)[-1]) * self.sigmoid_prime((self.z)[-1])\n",
        "        \n",
        "        \n",
        "        dCdW_final_layer = np.dot(delta, self.a[-2].T)\n",
        "        \n",
        "        w_i_grad[-1] = dCdW_final_layer\n",
        "        b_i_grad[-1] = delta\n",
        "\n",
        "        for i in range(2, self.layers):\n",
        "            z_current = self.z[-i]\n",
        "\n",
        "\n",
        "            delta = np.dot(self.weights[-i + 1].T, delta) * self.sigmoid_prime(z_current)\n",
        "\n",
        "            dW = np.dot(delta, self.a[-i - 1].T)\n",
        "\n",
        "            w_i_grad[-i] = dW\n",
        "            b_i_grad[-i] = delta\n",
        "        return w_i_grad, b_i_grad\n",
        "\n",
        "    def update_weights_and_biases(self, w_grad, b_grad, learning_rate):        \n",
        "\n",
        "        #apply gradients\n",
        "        for i in range(len(self.weights)): \n",
        "            self.weights[i] -= learning_rate*w_grad[i]\n",
        "            self.biases[i] -= learning_rate*b_grad[i]\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9P_bc193FUT"
      },
      "source": [
        "#Testing on MNIST using SGD\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2]) / 255\n",
        "\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\n",
        "\n",
        "def prepare_mini_batches(minibatch_size):\n",
        "    random_indexes = []\n",
        "    indexes = np.random.choice(X_train.shape[0], minibatch_size, replace=False)\n",
        "    random_indexes.append(indexes) \n",
        "    return random_indexes\n",
        "\n",
        "def one_hot_encode(length, index):\n",
        "    output = [0 for i in range(length)]\n",
        "    output[index] = 1\n",
        "    return np.asarray(output).reshape(length, 1)\n",
        "\n",
        "\n",
        "def compute_test_loss(test_set, net):\n",
        "    total_loss = []\n",
        "    for i in range(len(X_test)):\n",
        "        loss = net.feedforward(X_test[i].reshape(X_test[i].shape[0], 1), one_hot_encode(10 ,Y_test[i]))\n",
        "        total_loss.append(loss)\n",
        "    return sum(total_loss)/len(total_loss)\n",
        "\n",
        "#instantiate neural net\n",
        "nn = NeuralNet([784, 392, 196, 10])\n",
        "\n",
        "\n",
        "#training loop\n",
        "epochs = 300\n",
        "mini_size = 32\n",
        "for i in range(epochs):\n",
        "    random_indexes = prepare_mini_batches(mini_size)\n",
        "\n",
        "    for minibatch in random_indexes:\n",
        "        grads_w = [np.zeros(element.shape) for element in nn.weights]\n",
        "        grads_b = [np.zeros(element.shape) for element in nn.biases]\n",
        "        cost = []\n",
        "        for j in minibatch:\n",
        "            cost_iter = nn.feedforward(X_train[j].reshape(X_train[j].shape[0], 1), one_hot_encode(10 ,Y_train[j]))\n",
        "            cost.append(cost_iter)\n",
        "            grad_w, grad_b = nn.compute_gradients_backprop(one_hot_encode(10, Y_train[j]))\n",
        "\n",
        "            for i in range(len(grad_w)):\n",
        "                grads_w[i] += grad_w[i]\n",
        "                grads_b[i] += grad_b[i]\n",
        "        grads_w = [grads_w[i]/mini_size for i in range(len(grad_w))]\n",
        "        grads_b = [grads_b[i]/mini_size for i in range(len(grad_b))]\n",
        "\n",
        "        #update weights\n",
        "        print(\"Cost for batch  = \" + str(sum(cost)/len(cost)))\n",
        "        print(\"updating weights\")\n",
        "        nn.update_weights_and_biases(grads_w, grads_b, 0.01)\n",
        "        print(\"test loss = \" + str(compute_test_loss(X_test, nn)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}