{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vectorized NN",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN8URvaflG0483eGVfWhiYd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AditMeh/deep-learning/blob/main/Vectorized_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea6j52F0R95W"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "def compute_dw(delta, a):\r\n",
        "    \"\"\"\r\n",
        "    This function contains a vectorized implementation of computing the\r\n",
        "    gradients of the weights given the error (delta) of layer L and the\r\n",
        "    activations of layer L-1\r\n",
        "    \"\"\"\r\n",
        "        batch_size, num_prev_neurons = a.shape\r\n",
        "        _, num_curr_neurons = delta.shape\r\n",
        "\r\n",
        "        tile_delta = np.tile(delta, (1, num_prev_neurons)).reshape((batch_size, num_prev_neurons, num_curr_neurons))\r\n",
        "        tile_delta = np.transpose(tile_delta, (0, 2, 1))\r\n",
        "\r\n",
        "\r\n",
        "        tile_activations = np.tile(a, (1, num_curr_neurons)).reshape(batch_size, num_curr_neurons, num_prev_neurons)\r\n",
        "\r\n",
        "\r\n",
        "        dW =  tile_delta * tile_activations \r\n",
        "        dW = np.transpose(dW, (0, 2, 1))\r\n",
        "        return dW"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN03nBpqQNrg"
      },
      "source": [
        "class NeuralNet:\r\n",
        "    def __init__(self, layer_list):\r\n",
        "        self.layers = len(layer_list)\r\n",
        "        self.weights = []\r\n",
        "        self.biases = []\r\n",
        "\r\n",
        "\r\n",
        "        #randomly initialize weights and biases (scale each random value by 0.01 to prevent vanishing gradient due the saturation of sigmoid)\r\n",
        "        for layer_index in range(1, len(layer_list)):\r\n",
        "            self.weights.append(np.random.rand(layer_list[layer_index - 1], layer_list[layer_index])*0.01)\r\n",
        "            self.biases.append(np.random.rand(layer_list[layer_index],)*0.01)\r\n",
        "        \r\n",
        "        print(\"Weights\")\r\n",
        "        print([element.shape for element in self.weights])\r\n",
        "\r\n",
        "        print(\"\\n\" + \"Biases\")\r\n",
        "        print([element.shape for element in self.biases])\r\n",
        "    \r\n",
        "    def sigmoid(self, x):\r\n",
        "        return 1/(1 + (np.e)**(-x))\r\n",
        "    def sigmoid_prime(self, x):\r\n",
        "        return self.sigmoid(x)*(1 - self.sigmoid(x))\r\n",
        "    def compute_cost(self, real, predicted, batch_size):\r\n",
        "        return (1/batch_size) * (np.sum(1/2*(predicted - real)**2))\r\n",
        "    def cost_derivative(self, real, predicted):\r\n",
        "        return (predicted - real)\r\n",
        "    \r\n",
        "    def feedforward(self, x, output):\r\n",
        "       self.x = x\r\n",
        "       self.output = output\r\n",
        "\r\n",
        "       #Activation of the input layer is equivalent to the inputs that are passed in\r\n",
        "       current_a = self.x\r\n",
        "\r\n",
        "       self.z = []\r\n",
        "       self.a = [self.x]\r\n",
        "       for w_i, b_i in zip(self.weights, self.biases):\r\n",
        "           z_i = np.dot(current_a, w_i) + b_i\r\n",
        "\r\n",
        "           self.z.append(z_i)\r\n",
        "           current_a = self.sigmoid(z_i)\r\n",
        "           self.a.append(current_a)\r\n",
        "\r\n",
        "       return self.compute_cost(output, current_a, self.x.shape[0])\r\n",
        "    def backward(self):\r\n",
        "        #print([thing.shape for thing in self.a])\r\n",
        "        w_i_grad = [np.zeros(element.shape) for element in self.weights]\r\n",
        "        b_i_grad = [np.zeros(element.shape) for element in self.biases]\r\n",
        "\r\n",
        "        # compute final layer error\r\n",
        "        delta  = self.cost_derivative(self.output, (self.a)[-1]) * self.sigmoid_prime((self.z)[-1])\r\n",
        "\r\n",
        "        w_i_grad[-1] = compute_dw(delta, self.a[-2])\r\n",
        "        b_i_grad[-1] = delta\r\n",
        "        # compute  \r\n",
        "        for i in range(2, self.layers):\r\n",
        "            z_current = self.z[-i]\r\n",
        "\r\n",
        "            # delta non-final layer error\r\n",
        "\r\n",
        "            delta = np.dot(delta, (self.weights[-i + 1]).T) * self.sigmoid_prime(z_current)\r\n",
        "            #print(delta.shape)\r\n",
        "\r\n",
        "            # Computing dC/Dw\r\n",
        "\r\n",
        "            dW = compute_dw(delta, self.a[-i - 1])\r\n",
        "\r\n",
        "            w_i_grad[-i] = dW\r\n",
        "            b_i_grad[-i] = delta\r\n",
        "\r\n",
        "\r\n",
        "        return w_i_grad, b_i_grad\r\n",
        "    \r\n",
        "    def update_weights_and_biases(self, w_grad, b_grad, learning_rate):        \r\n",
        "\r\n",
        "        batch_size = w_grad[0].shape[0]\r\n",
        "        #apply gradients to current weights\r\n",
        "        for i in range(len(self.weights)): \r\n",
        "            self.weights[i] -= learning_rate*(np.sum(w_grad[i], axis = 0)/batch_size)\r\n",
        "            self.biases[i] -= learning_rate*(np.sum(b_grad[i], axis = 0)/batch_size)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0SMJOOsQnIq",
        "outputId": "53994b5f-18a8-4679-d398-eb1428f40cce"
      },
      "source": [
        "\"\"\"\r\n",
        "Test run\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "input = np.ones((10, 50))\r\n",
        "\r\n",
        "output = np.ones((10, 1))\r\n",
        "\r\n",
        "nn = NeuralNet([50, 20, 30, 2])\r\n",
        "\r\n",
        "nn.feedforward(input, output)\r\n"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights\n",
            "[(50, 20), (20, 30), (30, 2)]\n",
            "\n",
            "Biases\n",
            "[(20,), (30,), (2,)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.23022713583367557"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viyDbUrxSAhp",
        "outputId": "23f9ee54-c3a5-410b-94af-d37a4d3d98ea"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\r\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\r\n",
        "\r\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2]) / 255\r\n",
        "\r\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])\r\n",
        "\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "\r\n",
        "def prepare_mini_batches(minibatch_size: int):\r\n",
        "    \"\"\"\r\n",
        "    Takes in a minibatch size parameter and generates random indexes from the\r\n",
        "    training set (X_train). Returns a list of indexes of length minibatch_size.\r\n",
        "    \"\"\"\r\n",
        "    random_indexes = np.random.choice(X_train.shape[0], minibatch_size, replace=False)\r\n",
        "    return random_indexes\r\n",
        "\r\n",
        "# Creating one hot encoder\r\n",
        "encoder = OneHotEncoder()\r\n",
        "encoder.fit(Y_train.reshape(-1, 1))\r\n",
        "\r\n",
        "\r\n",
        "#Initializing the neural net\r\n",
        "nn = NeuralNet([784, 362, 100, 10])\r\n",
        "\r\n",
        "#training loop\r\n",
        "epochs = 300\r\n",
        "mini_size = 32\r\n",
        "lr = 0.01\r\n",
        "\r\n",
        "\r\n",
        "history = []\r\n",
        "for i in range(epochs):\r\n",
        "\r\n",
        "    print(\"EPOCH: \" + str(i + 1))\r\n",
        "\r\n",
        "    #Generates a randomly chosen set of training indexes per epoch\r\n",
        "    random_indexes = prepare_mini_batches(mini_size)\r\n",
        "    cost_iter = nn.feedforward(X_train[random_indexes], encoder.transform(Y_train[random_indexes].reshape(-1, 1)).toarray())\r\n",
        "    history.append(cost_iter)\r\n",
        "    print(\"Cost: \" + str(cost_iter))\r\n",
        "    grads_w, grads_b = nn.backward()\r\n",
        "\r\n",
        "    nn.update_weights_and_biases(grads_w, grads_b, learning_rate=lr)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights\n",
            "[(784, 362), (362, 100), (100, 10)]\n",
            "\n",
            "Biases\n",
            "[(362,), (100,), (10,)]\n",
            "EPOCH: 1\n",
            "Cost: 1.6698500335035253\n",
            "EPOCH: 2\n",
            "Cost: 1.589720480992647\n",
            "EPOCH: 3\n",
            "Cost: 1.509052306606034\n",
            "EPOCH: 4\n",
            "Cost: 1.436488411567415\n",
            "EPOCH: 5\n",
            "Cost: 1.366004924253934\n",
            "EPOCH: 6\n",
            "Cost: 1.3003181411285674\n",
            "EPOCH: 7\n",
            "Cost: 1.2397910704392778\n",
            "EPOCH: 8\n",
            "Cost: 1.18219867201642\n",
            "EPOCH: 9\n",
            "Cost: 1.1309671768764034\n",
            "EPOCH: 10\n",
            "Cost: 1.0825925318740313\n",
            "EPOCH: 11\n",
            "Cost: 1.0384078390526619\n",
            "EPOCH: 12\n",
            "Cost: 0.9956272237719663\n",
            "EPOCH: 13\n",
            "Cost: 0.9586298997881864\n",
            "EPOCH: 14\n",
            "Cost: 0.9223531381256913\n",
            "EPOCH: 15\n",
            "Cost: 0.8946765619155291\n",
            "EPOCH: 16\n",
            "Cost: 0.8628365934907575\n",
            "EPOCH: 17\n",
            "Cost: 0.837751338694471\n",
            "EPOCH: 18\n",
            "Cost: 0.8105525695541862\n",
            "EPOCH: 19\n",
            "Cost: 0.7876482517250782\n",
            "EPOCH: 20\n",
            "Cost: 0.7661658611797209\n",
            "EPOCH: 21\n",
            "Cost: 0.7546859676831286\n",
            "EPOCH: 22\n",
            "Cost: 0.7298468066630636\n",
            "EPOCH: 23\n",
            "Cost: 0.7137086796675\n",
            "EPOCH: 24\n",
            "Cost: 0.7035379284274981\n",
            "EPOCH: 25\n",
            "Cost: 0.685262004875155\n",
            "EPOCH: 26\n",
            "Cost: 0.6733204355785151\n",
            "EPOCH: 27\n",
            "Cost: 0.6614666800265064\n",
            "EPOCH: 28\n",
            "Cost: 0.6515527729799304\n",
            "EPOCH: 29\n",
            "Cost: 0.637211267753415\n",
            "EPOCH: 30\n",
            "Cost: 0.6309823196835378\n",
            "EPOCH: 31\n",
            "Cost: 0.62331846169373\n",
            "EPOCH: 32\n",
            "Cost: 0.6155391862756252\n",
            "EPOCH: 33\n",
            "Cost: 0.6064191290129972\n",
            "EPOCH: 34\n",
            "Cost: 0.5974873790568065\n",
            "EPOCH: 35\n",
            "Cost: 0.5934091978524825\n",
            "EPOCH: 36\n",
            "Cost: 0.5843767576572686\n",
            "EPOCH: 37\n",
            "Cost: 0.5783221309998818\n",
            "EPOCH: 38\n",
            "Cost: 0.5725223344007031\n",
            "EPOCH: 39\n",
            "Cost: 0.5675054036538542\n",
            "EPOCH: 40\n",
            "Cost: 0.5628786492866023\n",
            "EPOCH: 41\n",
            "Cost: 0.558590404640051\n",
            "EPOCH: 42\n",
            "Cost: 0.5556250993509761\n",
            "EPOCH: 43\n",
            "Cost: 0.5502727930512228\n",
            "EPOCH: 44\n",
            "Cost: 0.5457366305075304\n",
            "EPOCH: 45\n",
            "Cost: 0.5423859409250911\n",
            "EPOCH: 46\n",
            "Cost: 0.5393966808029576\n",
            "EPOCH: 47\n",
            "Cost: 0.5355626882513882\n",
            "EPOCH: 48\n",
            "Cost: 0.5351141784465726\n",
            "EPOCH: 49\n",
            "Cost: 0.5290613407049745\n",
            "EPOCH: 50\n",
            "Cost: 0.5256360254641522\n",
            "EPOCH: 51\n",
            "Cost: 0.5239404378420067\n",
            "EPOCH: 52\n",
            "Cost: 0.5210591076151723\n",
            "EPOCH: 53\n",
            "Cost: 0.5160212257345228\n",
            "EPOCH: 54\n",
            "Cost: 0.5154752507120828\n",
            "EPOCH: 55\n",
            "Cost: 0.5116910725852049\n",
            "EPOCH: 56\n",
            "Cost: 0.511817438632789\n",
            "EPOCH: 57\n",
            "Cost: 0.5103328129262522\n",
            "EPOCH: 58\n",
            "Cost: 0.5058412868790271\n",
            "EPOCH: 59\n",
            "Cost: 0.5073866854763194\n",
            "EPOCH: 60\n",
            "Cost: 0.5043907390041005\n",
            "EPOCH: 61\n",
            "Cost: 0.5031458280872736\n",
            "EPOCH: 62\n",
            "Cost: 0.4999253260135893\n",
            "EPOCH: 63\n",
            "Cost: 0.49812965445360136\n",
            "EPOCH: 64\n",
            "Cost: 0.4971207112525086\n",
            "EPOCH: 65\n",
            "Cost: 0.4952725226520951\n",
            "EPOCH: 66\n",
            "Cost: 0.4946347129370262\n",
            "EPOCH: 67\n",
            "Cost: 0.49231726880581284\n",
            "EPOCH: 68\n",
            "Cost: 0.4920675876473992\n",
            "EPOCH: 69\n",
            "Cost: 0.490813702436958\n",
            "EPOCH: 70\n",
            "Cost: 0.48782174259044236\n",
            "EPOCH: 71\n",
            "Cost: 0.48958146882110964\n",
            "EPOCH: 72\n",
            "Cost: 0.4873332478429459\n",
            "EPOCH: 73\n",
            "Cost: 0.4867969275668841\n",
            "EPOCH: 74\n",
            "Cost: 0.48490141736316217\n",
            "EPOCH: 75\n",
            "Cost: 0.4842139506356018\n",
            "EPOCH: 76\n",
            "Cost: 0.4844107399741218\n",
            "EPOCH: 77\n",
            "Cost: 0.4827908593267001\n",
            "EPOCH: 78\n",
            "Cost: 0.48192132278342414\n",
            "EPOCH: 79\n",
            "Cost: 0.4811245943514482\n",
            "EPOCH: 80\n",
            "Cost: 0.4802524565597252\n",
            "EPOCH: 81\n",
            "Cost: 0.47879497638541235\n",
            "EPOCH: 82\n",
            "Cost: 0.4798248800106519\n",
            "EPOCH: 83\n",
            "Cost: 0.47732834272118013\n",
            "EPOCH: 84\n",
            "Cost: 0.47688955069068234\n",
            "EPOCH: 85\n",
            "Cost: 0.4768259222182576\n",
            "EPOCH: 86\n",
            "Cost: 0.47609541361682506\n",
            "EPOCH: 87\n",
            "Cost: 0.475116337656286\n",
            "EPOCH: 88\n",
            "Cost: 0.47462186119829436\n",
            "EPOCH: 89\n",
            "Cost: 0.47313453384244913\n",
            "EPOCH: 90\n",
            "Cost: 0.47286343539508857\n",
            "EPOCH: 91\n",
            "Cost: 0.47258661612556\n",
            "EPOCH: 92\n",
            "Cost: 0.4731901197280911\n",
            "EPOCH: 93\n",
            "Cost: 0.47163876374453356\n",
            "EPOCH: 94\n",
            "Cost: 0.47077197614236543\n",
            "EPOCH: 95\n",
            "Cost: 0.4718487962143088\n",
            "EPOCH: 96\n",
            "Cost: 0.4694039628081859\n",
            "EPOCH: 97\n",
            "Cost: 0.4689793814018205\n",
            "EPOCH: 98\n",
            "Cost: 0.4690000384979479\n",
            "EPOCH: 99\n",
            "Cost: 0.470320801522402\n",
            "EPOCH: 100\n",
            "Cost: 0.4687064112080611\n",
            "EPOCH: 101\n",
            "Cost: 0.4675604845381406\n",
            "EPOCH: 102\n",
            "Cost: 0.46778439325918053\n",
            "EPOCH: 103\n",
            "Cost: 0.46764927963866176\n",
            "EPOCH: 104\n",
            "Cost: 0.467514333772963\n",
            "EPOCH: 105\n",
            "Cost: 0.4663914970415375\n",
            "EPOCH: 106\n",
            "Cost: 0.46674826960605276\n",
            "EPOCH: 107\n",
            "Cost: 0.46633790478750536\n",
            "EPOCH: 108\n",
            "Cost: 0.4648706207874653\n",
            "EPOCH: 109\n",
            "Cost: 0.46548652986223704\n",
            "EPOCH: 110\n",
            "Cost: 0.46486009360632097\n",
            "EPOCH: 111\n",
            "Cost: 0.4646208930515261\n",
            "EPOCH: 112\n",
            "Cost: 0.4644066144447935\n",
            "EPOCH: 113\n",
            "Cost: 0.4645277525088324\n",
            "EPOCH: 114\n",
            "Cost: 0.46312845164872096\n",
            "EPOCH: 115\n",
            "Cost: 0.4629763251339554\n",
            "EPOCH: 116\n",
            "Cost: 0.46310603183162785\n",
            "EPOCH: 117\n",
            "Cost: 0.4620759017838495\n",
            "EPOCH: 118\n",
            "Cost: 0.4631979307161013\n",
            "EPOCH: 119\n",
            "Cost: 0.46183343914968733\n",
            "EPOCH: 120\n",
            "Cost: 0.461736746999609\n",
            "EPOCH: 121\n",
            "Cost: 0.4617046577437755\n",
            "EPOCH: 122\n",
            "Cost: 0.46070633815641604\n",
            "EPOCH: 123\n",
            "Cost: 0.4615049659129018\n",
            "EPOCH: 124\n",
            "Cost: 0.4613727584997508\n",
            "EPOCH: 125\n",
            "Cost: 0.4605328268560097\n",
            "EPOCH: 126\n",
            "Cost: 0.4601708438034098\n",
            "EPOCH: 127\n",
            "Cost: 0.4602410336856098\n",
            "EPOCH: 128\n",
            "Cost: 0.4601590292472437\n",
            "EPOCH: 129\n",
            "Cost: 0.46006269780148046\n",
            "EPOCH: 130\n",
            "Cost: 0.4599167909561275\n",
            "EPOCH: 131\n",
            "Cost: 0.45931030773095316\n",
            "EPOCH: 132\n",
            "Cost: 0.4590950111159233\n",
            "EPOCH: 133\n",
            "Cost: 0.4588709145982163\n",
            "EPOCH: 134\n",
            "Cost: 0.4587301387287357\n",
            "EPOCH: 135\n",
            "Cost: 0.45882527527834455\n",
            "EPOCH: 136\n",
            "Cost: 0.4596436887819748\n",
            "EPOCH: 137\n",
            "Cost: 0.45838998455056945\n",
            "EPOCH: 138\n",
            "Cost: 0.4597447010749766\n",
            "EPOCH: 139\n",
            "Cost: 0.4576428770538469\n",
            "EPOCH: 140\n",
            "Cost: 0.45849974503133506\n",
            "EPOCH: 141\n",
            "Cost: 0.45858021576679375\n",
            "EPOCH: 142\n",
            "Cost: 0.45761218367829765\n",
            "EPOCH: 143\n",
            "Cost: 0.457796913157029\n",
            "EPOCH: 144\n",
            "Cost: 0.45765812752548957\n",
            "EPOCH: 145\n",
            "Cost: 0.45707732526004985\n",
            "EPOCH: 146\n",
            "Cost: 0.4565152042148847\n",
            "EPOCH: 147\n",
            "Cost: 0.457534723675344\n",
            "EPOCH: 148\n",
            "Cost: 0.4562251771957899\n",
            "EPOCH: 149\n",
            "Cost: 0.45631616732134683\n",
            "EPOCH: 150\n",
            "Cost: 0.4568673045193662\n",
            "EPOCH: 151\n",
            "Cost: 0.4575727709610822\n",
            "EPOCH: 152\n",
            "Cost: 0.45641603312010126\n",
            "EPOCH: 153\n",
            "Cost: 0.4559056351207045\n",
            "EPOCH: 154\n",
            "Cost: 0.4556790743979331\n",
            "EPOCH: 155\n",
            "Cost: 0.4573358575094301\n",
            "EPOCH: 156\n",
            "Cost: 0.45689680247586967\n",
            "EPOCH: 157\n",
            "Cost: 0.45603716891714186\n",
            "EPOCH: 158\n",
            "Cost: 0.4554284672082342\n",
            "EPOCH: 159\n",
            "Cost: 0.4565174781663285\n",
            "EPOCH: 160\n",
            "Cost: 0.45592505533999106\n",
            "EPOCH: 161\n",
            "Cost: 0.4561002972724211\n",
            "EPOCH: 162\n",
            "Cost: 0.45468455741604685\n",
            "EPOCH: 163\n",
            "Cost: 0.45553489414741566\n",
            "EPOCH: 164\n",
            "Cost: 0.45548397516907047\n",
            "EPOCH: 165\n",
            "Cost: 0.454857486678148\n",
            "EPOCH: 166\n",
            "Cost: 0.45522670803723897\n",
            "EPOCH: 167\n",
            "Cost: 0.4552100254178366\n",
            "EPOCH: 168\n",
            "Cost: 0.45471166850124467\n",
            "EPOCH: 169\n",
            "Cost: 0.4550995676446346\n",
            "EPOCH: 170\n",
            "Cost: 0.4557134172059098\n",
            "EPOCH: 171\n",
            "Cost: 0.4552499142420909\n",
            "EPOCH: 172\n",
            "Cost: 0.45557070609890005\n",
            "EPOCH: 173\n",
            "Cost: 0.45469167297594326\n",
            "EPOCH: 174\n",
            "Cost: 0.4550299279869498\n",
            "EPOCH: 175\n",
            "Cost: 0.4547683982360684\n",
            "EPOCH: 176\n",
            "Cost: 0.4542591975111991\n",
            "EPOCH: 177\n",
            "Cost: 0.4541518769073031\n",
            "EPOCH: 178\n",
            "Cost: 0.45381236679365267\n",
            "EPOCH: 179\n",
            "Cost: 0.45387845461266946\n",
            "EPOCH: 180\n",
            "Cost: 0.45428557941234937\n",
            "EPOCH: 181\n",
            "Cost: 0.4538515647752256\n",
            "EPOCH: 182\n",
            "Cost: 0.4545898998896416\n",
            "EPOCH: 183\n",
            "Cost: 0.45331535103667076\n",
            "EPOCH: 184\n",
            "Cost: 0.4532831864706677\n",
            "EPOCH: 185\n",
            "Cost: 0.45310120922772656\n",
            "EPOCH: 186\n",
            "Cost: 0.4531781156953648\n",
            "EPOCH: 187\n",
            "Cost: 0.45360161290821144\n",
            "EPOCH: 188\n",
            "Cost: 0.4539713918624206\n",
            "EPOCH: 189\n",
            "Cost: 0.45374560249109963\n",
            "EPOCH: 190\n",
            "Cost: 0.4539707040105171\n",
            "EPOCH: 191\n",
            "Cost: 0.45346816106938564\n",
            "EPOCH: 192\n",
            "Cost: 0.45430287183710877\n",
            "EPOCH: 193\n",
            "Cost: 0.45325962867296943\n",
            "EPOCH: 194\n",
            "Cost: 0.45386463950309475\n",
            "EPOCH: 195\n",
            "Cost: 0.4532492175031084\n",
            "EPOCH: 196\n",
            "Cost: 0.4535795645249436\n",
            "EPOCH: 197\n",
            "Cost: 0.45291924383749116\n",
            "EPOCH: 198\n",
            "Cost: 0.4530369805052212\n",
            "EPOCH: 199\n",
            "Cost: 0.45253916427724156\n",
            "EPOCH: 200\n",
            "Cost: 0.4523998748513657\n",
            "EPOCH: 201\n",
            "Cost: 0.45326879855102065\n",
            "EPOCH: 202\n",
            "Cost: 0.45307605472888957\n",
            "EPOCH: 203\n",
            "Cost: 0.45260733703173106\n",
            "EPOCH: 204\n",
            "Cost: 0.45289062486954945\n",
            "EPOCH: 205\n",
            "Cost: 0.4528214460689992\n",
            "EPOCH: 206\n",
            "Cost: 0.452519068362725\n",
            "EPOCH: 207\n",
            "Cost: 0.45312456584633176\n",
            "EPOCH: 208\n",
            "Cost: 0.45203455863040903\n",
            "EPOCH: 209\n",
            "Cost: 0.4514756692905485\n",
            "EPOCH: 210\n",
            "Cost: 0.4537027475544355\n",
            "EPOCH: 211\n",
            "Cost: 0.4528255586624274\n",
            "EPOCH: 212\n",
            "Cost: 0.45295269169873903\n",
            "EPOCH: 213\n",
            "Cost: 0.4521834847792957\n",
            "EPOCH: 214\n",
            "Cost: 0.4529524704703193\n",
            "EPOCH: 215\n",
            "Cost: 0.4535360196141133\n",
            "EPOCH: 216\n",
            "Cost: 0.45197647738042634\n",
            "EPOCH: 217\n",
            "Cost: 0.45180594124458395\n",
            "EPOCH: 218\n",
            "Cost: 0.4524612008525098\n",
            "EPOCH: 219\n",
            "Cost: 0.45216542210182614\n",
            "EPOCH: 220\n",
            "Cost: 0.45204086244070724\n",
            "EPOCH: 221\n",
            "Cost: 0.4528492100125424\n",
            "EPOCH: 222\n",
            "Cost: 0.45259606469048097\n",
            "EPOCH: 223\n",
            "Cost: 0.45199618262246416\n",
            "EPOCH: 224\n",
            "Cost: 0.45184851313164487\n",
            "EPOCH: 225\n",
            "Cost: 0.4514863941375694\n",
            "EPOCH: 226\n",
            "Cost: 0.4515779272925694\n",
            "EPOCH: 227\n",
            "Cost: 0.4528281872957354\n",
            "EPOCH: 228\n",
            "Cost: 0.4512834830983774\n",
            "EPOCH: 229\n",
            "Cost: 0.4514170701998794\n",
            "EPOCH: 230\n",
            "Cost: 0.4523297224382411\n",
            "EPOCH: 231\n",
            "Cost: 0.45164531285729353\n",
            "EPOCH: 232\n",
            "Cost: 0.451597028373637\n",
            "EPOCH: 233\n",
            "Cost: 0.45205503968489424\n",
            "EPOCH: 234\n",
            "Cost: 0.45159967071355955\n",
            "EPOCH: 235\n",
            "Cost: 0.45179438058201415\n",
            "EPOCH: 236\n",
            "Cost: 0.45172142106978985\n",
            "EPOCH: 237\n",
            "Cost: 0.4513540707837048\n",
            "EPOCH: 238\n",
            "Cost: 0.45131689393042984\n",
            "EPOCH: 239\n",
            "Cost: 0.45284291650538694\n",
            "EPOCH: 240\n",
            "Cost: 0.45128983171532205\n",
            "EPOCH: 241\n",
            "Cost: 0.451769371299053\n",
            "EPOCH: 242\n",
            "Cost: 0.45199682066094166\n",
            "EPOCH: 243\n",
            "Cost: 0.451402527891558\n",
            "EPOCH: 244\n",
            "Cost: 0.45098410720177967\n",
            "EPOCH: 245\n",
            "Cost: 0.45094281259812474\n",
            "EPOCH: 246\n",
            "Cost: 0.4517717169426727\n",
            "EPOCH: 247\n",
            "Cost: 0.45244954476560983\n",
            "EPOCH: 248\n",
            "Cost: 0.45056054833347137\n",
            "EPOCH: 249\n",
            "Cost: 0.45061397295187744\n",
            "EPOCH: 250\n",
            "Cost: 0.4522042524413436\n",
            "EPOCH: 251\n",
            "Cost: 0.4518381332640625\n",
            "EPOCH: 252\n",
            "Cost: 0.45149321193931913\n",
            "EPOCH: 253\n",
            "Cost: 0.4509027481786017\n",
            "EPOCH: 254\n",
            "Cost: 0.45050891961484063\n",
            "EPOCH: 255\n",
            "Cost: 0.45128429871837306\n",
            "EPOCH: 256\n",
            "Cost: 0.45101170549536407\n",
            "EPOCH: 257\n",
            "Cost: 0.4511813924222801\n",
            "EPOCH: 258\n",
            "Cost: 0.45113118431513893\n",
            "EPOCH: 259\n",
            "Cost: 0.45132172200635723\n",
            "EPOCH: 260\n",
            "Cost: 0.45166863207026575\n",
            "EPOCH: 261\n",
            "Cost: 0.4503050717864532\n",
            "EPOCH: 262\n",
            "Cost: 0.45099730582226605\n",
            "EPOCH: 263\n",
            "Cost: 0.4508912050120842\n",
            "EPOCH: 264\n",
            "Cost: 0.4511601553004485\n",
            "EPOCH: 265\n",
            "Cost: 0.44992889295220995\n",
            "EPOCH: 266\n",
            "Cost: 0.45063733871899175\n",
            "EPOCH: 267\n",
            "Cost: 0.4525838729259537\n",
            "EPOCH: 268\n",
            "Cost: 0.45135390064722464\n",
            "EPOCH: 269\n",
            "Cost: 0.44885589195229025\n",
            "EPOCH: 270\n",
            "Cost: 0.4507016544311626\n",
            "EPOCH: 271\n",
            "Cost: 0.45222072017261206\n",
            "EPOCH: 272\n",
            "Cost: 0.4512231541055791\n",
            "EPOCH: 273\n",
            "Cost: 0.45123206571635943\n",
            "EPOCH: 274\n",
            "Cost: 0.45114181897888883\n",
            "EPOCH: 275\n",
            "Cost: 0.4493684681845348\n",
            "EPOCH: 276\n",
            "Cost: 0.45185835791395323\n",
            "EPOCH: 277\n",
            "Cost: 0.4501769918925107\n",
            "EPOCH: 278\n",
            "Cost: 0.45087611974323905\n",
            "EPOCH: 279\n",
            "Cost: 0.4509514239486739\n",
            "EPOCH: 280\n",
            "Cost: 0.4515504136261353\n",
            "EPOCH: 281\n",
            "Cost: 0.45265874329419575\n",
            "EPOCH: 282\n",
            "Cost: 0.45099352639047063\n",
            "EPOCH: 283\n",
            "Cost: 0.45192469744547326\n",
            "EPOCH: 284\n",
            "Cost: 0.4504383141648648\n",
            "EPOCH: 285\n",
            "Cost: 0.4506252335041918\n",
            "EPOCH: 286\n",
            "Cost: 0.45152090598467526\n",
            "EPOCH: 287\n",
            "Cost: 0.45029548782644163\n",
            "EPOCH: 288\n",
            "Cost: 0.45097804374607653\n",
            "EPOCH: 289\n",
            "Cost: 0.4513852381979589\n",
            "EPOCH: 290\n",
            "Cost: 0.45139139669054507\n",
            "EPOCH: 291\n",
            "Cost: 0.44993020244423004\n",
            "EPOCH: 292\n",
            "Cost: 0.4518763504905339\n",
            "EPOCH: 293\n",
            "Cost: 0.45087506607845207\n",
            "EPOCH: 294\n",
            "Cost: 0.4515624639051874\n",
            "EPOCH: 295\n",
            "Cost: 0.45034096128315904\n",
            "EPOCH: 296\n",
            "Cost: 0.4517604757019806\n",
            "EPOCH: 297\n",
            "Cost: 0.4513161999828323\n",
            "EPOCH: 298\n",
            "Cost: 0.4509747513628063\n",
            "EPOCH: 299\n",
            "Cost: 0.44983319892652796\n",
            "EPOCH: 300\n",
            "Cost: 0.4500166301124625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ahLfF2UVFVpO",
        "outputId": "ecd6588d-9d2b-456f-c0bc-c1b9136c0c1e"
      },
      "source": [
        "# Displaying the history\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "x = [(i + 1) for i in range(len(history))]\r\n",
        "\r\n",
        "plt.plot(x, history)\r\n",
        "plt.title(\"Neural network trained on MNIST: Epochs vs Loss\")\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"MSE Loss\");\r\n"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxcVZn/8c9Ta6/pdLo7C1lIQiABWRQDghsI4wwwKjPqCKgjOiijo6gzrqiDy4y/+bkLI47ibxDREUVFBxUEFwjjIEqCbAETQghkT3c66fTeXVXP7497OlQ6vSXp6uru+32/Xv3qukvd+9y6Vfe555x7zzV3R0RE4itR7gBERKS8lAhERGJOiUBEJOaUCEREYk6JQEQk5pQIRERiTolgkjGzT5jZd8odx5Ewszeb2W8ncH0vMbN1JVr2DWb2r6VYthwaM7vbzN5a7jimo9gnAjPbZGa7zKy6aNxbzezuMoY17szsbDPbUu44BhuPxOfu/+Puy8crpnIICcfN7MJB478Uxr85DL85DH9w0HxbzOzs8PqAz9TMLjSzB81sn5m1mNlvzGyJmX3NzDrCX5+Z9RcN3z6GePuK5u8ws4fG6/OY7KbDCVux2CeCIAm8p9QrMbNUqddRbuO9jRaJy/d0PfCmgYHwWb4OeHLQfK3AB82sdrQFmtky4EbgfUAdsAS4Fsi7+9vdvcbda4D/A3x/YNjdzx9DvJ8tmr/G3U8Zy0bK5BOXH9hoPge838xmDjXRzFaY2S/NrNXM1pnZ64qmHVBcHVwtEs7e3mlmTwBPhHFXm9nmcIa2xsxeMpYgB87qzex9oRSz3czeUjQ9a2afN7NnzGxnOOOrDKWd24Gjis7ejjKzbjNrDO/9qJnlzGxGGP4XM/tyeF1nZjeaWbOZPW1mHxs4OIft/d9w5rob+MQQcX/OzH5rZnWDxp8HfAS4qPiMMnymnzaz/wW6gKVm9hYze9zM2s1so5n9/eDPpWh4k5m938weNrM2M/u+mVUUTX9FOEPea2b3mtnJRdOeZ2YPhPV8H9j/viG2KxE+i6fD/rhxYBvNbHHY95eG/dFiZh8dZRf/FHixmdWH4fOAh4Edg+Z7HPgd8E+jLA/gucBT7v5rj7S7+4/c/ZkxvPewFG375Wa2LXxP3180PWtmXw7TtoXX2aLpxSWYJ8P3ZMDR4fvWbmZ3Fn1/K8zsO2a2O+zX+81szhCxfcjMfjho3NVmdk14/ebw/Wo3s6fM7A2Hsf2vMrO1IY67zez4QevfGpa/zszODeNPN7PVYZt3mtkXD3W9R0KJILIauBt4/+AJ4SD6S+C7wGzgYuCrZnbCISz/r4AXAAPvuZ/oBzorLPcHxQeqUcwlOrObD1wGXFt04Pi/wHFh2cvCPFe5eydwPrCt6OxtW4jjrPDes4CngRcVDa8Kr/89rHNpGP8mYH8CCtu2EZgDfHpgZDhQfgM4Gfhzd28r3hB3/wUHnokWn1H+LXA5UBvi2gW8ApgR1v0lMzt1hM/pdUQH0iVh/W8OMT0PuB74e6AB+Dpwazg4ZYCfAN8m2jc/AF4zwjreHP5eFj6bGuArg+Z5MbAcOBe4qvigMIQe4L+JvmMQfc43DjPvPwPvNbNZIywP4AFgRUjULzOzmlHmB8DMXmxme8cy7wheBhwL/DnwITP7szD+o8AZRN/TU4DTgY+F9Z5OtM0fAGYCLwU2FS3z9UT7fzaQ4dnf7KVE39GFRPv17UD3EDF9D7jAQmnKzJJE35Xvht/6NcD57l4LvBB48FA22MyOA24C3gs0AbcBPzWzjJktB94FnBaW/xdF23Y1cLW7zwCOAW4+lPUeKSWCZ10FXGFmTYPGvwLY5O7fdPecu/8R+BHwN4ew7H9z91Z37wZw9++4++6wvC8AWaKDxVj0A59y9353vw3oAJabmREdOP8xrKud6CB78QjLWgWcZVEVxMlEP4KzQlI6Dbgn/FAuBq4MZ5ObgC8QHagHbHP3fw/bM/DjSxP9IGYBr3T3rjFu34Ab3H1tWGa/u//c3Z8MZ7WrgDuBkUpS17j7NndvJTrTfm4YfznwdXf/vbvn3f1bQC/RgemMEPeXwzp/SJQsh/MG4IvuvtHdO4ArgYvtwOqxT7p7t7s/BDxEdOAbyY3AmywqnZ5FlJgO4u4PEp2gfGikhbn7RuBsopOCm4EWi+r3R0wI7v5bdx+yhFzk/eGsd+DvW4Omf9LdO939EeCbwCVh/BuIvsO73L0Z+CTPfp8uA65391+6e8Hdt7r7n4qW+U13Xx++Zzfz7H7tJ0oAy8J+XePu+4bYrqeJkuNfh1HnAF3ufl8YLgAnmlmlu29397WjfAaDXQT8PMTfD3weqCRKKnmi3/oJZpZ2903uPlDt1w8sM7NGd+8oimdCKBEE7v4o8DPgw4MmHQ28oPgLT/RFnnsIi99cPBCqLR4P1RZ7ic5kGse4rN3unisa7iI6E20CqoA1RXH+Iowfziqig8SpwCNEB5aziA6IG9x9d4grTXRWPuBpogPLkNsXLAMuJDoY9I1x24oN/szON7P7LKqe2wtcwMifWXF1ysBnBNH+fN+g/bkQOCr8bfUDe2Is3u7BjuLgzyVFVDIaLY4huftvifbZR4GfFSXWoVwFvGOoKpBBy7zP3V/n7k1EyfOlYflH6vPuPrPo79JB04v34dNEnxcM/bkNTFvIwW0ixYb7PL8N3AF8L1Q3fdbM0sMs47s8m5ReH4YJJeeLiEoT283s52a2YoRYhnLAtrl7gehzmO/uG4hKCp8AdpnZ98xsYLsvIyrN/ylUa73iENd7RJQIDvRx4G0cfJBbNegLX+Pu7wjTO4kOwAOGShD7DywWtQd8kKg4Wh/OutoAO8LYW4iKws8pirMuNAQeEEORe4lKIn9NtI2PAYuIDrID1UItRGcrRxe9bxGwdajtK/I4URH+9lAkHs5w3d8Wf2ZZolLY54E54TO7jcP7zDYDnx60P6vc/SZgOzA/lK4GLBphWds4+HPJATsPI65i3yFq3B2uWgiAcKZ8C4dwUHf3+8N7TjySAMdoYdHrRUSfFwz9uQ1M20xUNXJIQgnuk+5+AtHZ9ysoangf5AfA2Wa2gOi7/92i5dzh7i8H5gF/Ar5xiKEcsG3hu7SQ8Htx9++6+4vDPA58Jox/wt0vIary+gzwQyu6krHUlAiKhIz9feDdRaN/BhxnZn9rZunwd1pRXe+DwKvNrMqiKzQuG2U1tUQHi2YgZWZXEdV7H2nsBaIv7ZfMbDaAmc03s78Is+wEGqyowTZU16wB3smzB/57ic6IVoV58kRF8E+bWa2ZHU3USDnqpXPh4PoR4FdmNtyPeyew2Ea+MihDVKRuBnJmdj5RvfPh+AbwdjN7gUWqzewvQ53x74j2zbvDfn41Uf31cG4C/tGiSzGLr7zJjfCesbgGeDlwzxjm/SRRwh3uQocXm9nbir4TK4BXARNR9fDP4XfxnBDj98P4m4CPmVmTRY29V/Hs9+k/gbeY2bmhjWn+WM7KQ/vHSaEqcx/RyUthqHlDddTdRNVVT7n742EZcyxqqK4mqi7sGG4ZQcKiRuqBvyzRb+UvQ/xpooTeC9xrZsvN7JwwXw/RiVshrPuNZtYUfscDbTMjrXtcKREc7FPA/kwc6tr/nKiefBtR0fQzRAcmgC8BfUQHtG8B/zXK8u8gqrJZT1SE7GHoqpXD8SFgA3Cfme0DfkVoewhnjzcBG0OVyECRdBVR1c8fioZrOfAgdAVRyWcj8FuiM6jrxxJQqIP/FPAbM1s8xCw/CP93m9kDwyyjnSg53wzsISrO3zqW9Q+xrNVEpb6vhGVtIDQkhyqsV4fhVqJqgltGWNz1RFUS9wBPEe3LKw4nrkExtnq4ymcM8z4VYhju7HEv0YH/ETPrIPru/Rj47EjLtegmvY5RVv9BO/A+gpZB01cRfb6/JqpGujOM/1eiCzQeJqqSfCCMw93/QLgYgKikvIoDSw/DmQv8kCgJPB7e9+0R5v8u8GcUlQaIjof/RPQ7byWqJn3HwW/d7xKig/nA35Puvg54I9EFFi3AK4nayPqIjhn/N4zfQXT2f2VY1nnA2vCZXw1cPEq14LiyMXzXRETGLCT8p4D0OJSOZAKoRCAiEnNKBCIiMaeqIRGRmFOJQEQk5qZcJ2iNjY2+ePHicochIjKlrFmzpiXcVHiQKZcIFi9ezOrVq8sdhojIlGJmw94lr6ohEZGYUyIQEYk5JQIRkZhTIhARiTklAhGRmFMiEBGJOSUCEZGYi00i+NOOfXz+jnXs6Tych2WJiExfsUkEm1q6+MpdG9i6d8K6+BYRmRJikwhmVWcAaFWJQETkALFLBHu6lAhERIrFJhE0hESwu0OJQESkWGwSQV1lmoSpakhEZLDYJIJEwqivytCqqiERkQPEJhFA1E7QqqohEZEDxC8RqGpIROQAJUsEZna9me0ys0dHmOdsM3vQzNaa2apSxTJgVrWqhkREBitlieAG4LzhJprZTOCrwKvc/TnA35QwFkAlAhGRoZQsEbj7PUDrCLO8HrjF3Z8J8+8qVSwDGqoz7OnqI1/wUq9KRGTKKGcbwXFAvZndbWZrzOxNpV7hrOoM7tDW3V/qVYmITBnlfHh9Cng+cC5QCfzOzO5z9/WDZzSzy4HLARYtWnTYK6zf381E7/47jUVE4q6cJYItwB3u3unuLcA9wClDzeju17n7Sndf2dTUdNgrbKjOArq7WESkWDkTwX8DLzazlJlVAS8AHi/lCtXfkIjIwUpWNWRmNwFnA41mtgX4OJAGcPevufvjZvYL4GGgAPw/dx/2UtPxMJAIduvKIRGR/UqWCNz9kjHM8zngc6WKYbD66jSA7i4WESkSqzuLs6kktdmUSgQiIkVilQggunJIbQQiIs+KXSLQ3cUiIgeKXSJoqM7o8lERkSKxSwSzVDUkInKAWCaC3Z19uKu/IRERiGki6MsV6OzLlzsUEZFJIZaJAGCPGoxFRIAYJwLdSyAiEoltImjt7C1zJCIik0PsEsFAD6StnXomgYgIxDARzKpRiUBEpFjsEkF1JkkmmVAbgYhIELtEYGbRTWVKBCIiQAwTAai/IRGRYrFNBKoaEhGJxDYRqEQgIhJRIhARiblYJoKG6gztPTn6coVyhyIiUnaxTAT1A/0NqTtqEZF4JoKGgf6G9IAaEZGYJoKagW4mlAhERGKZCJpqo0TQ3NFT5khERMovlomgMfQ31Nyu/oZEREqWCMzsejPbZWaPjjLfaWaWM7PXliqWwWqyKSrSCVrURiAiUtISwQ3AeSPNYGZJ4DPAnSWMY6j10liTVYlARIQSJgJ3vwdoHWW2K4AfAbtKFcdwmmqVCEREoIxtBGY2H/hr4D/GMO/lZrbazFY3NzePy/qbarK0dCgRiIiUs7H4y8CH3H3U23vd/Tp3X+nuK5uamsZl5Y0qEYiIAJAq47pXAt8zM4BG4AIzy7n7TyZi5U01WVq7+sjlC6SSsbx4SkQEKGMicPclA6/N7AbgZxOVBCBqI3CPbiqbPaNiolYrIjLplCwRmNlNwNlAo5ltAT4OpAHc/WulWu9YNYa7i3e19yoRiEislSwRuPslhzDvm0sVx3CevbtY7QQiEm+xrRxvCiWCFjUYi0jMxTYRNNaGbiZUIhCRmIttIqjKpKjJpmhpVzcTIhJvsU0EEHU+pxKBiMRdrBNB1M2EuqIWkXiLfSJQD6QiEnexTgTqgVREJOaJoKkmS1t3P725fLlDEREpm1gngsZwU5keYi8icRbrRNBU1M2EiEhcxToRzJ4REsE+XTkkIvEV60QwN3Q2t1OJQERiLNaJoKEmSzJh7FAiEJEYi3UiSCaM2bVZdrSpjUBE4ivWiQBgbl0FO/Z1lzsMEZGyUSKYUcGONlUNiUh8xT4RzJlRwc59qhoSkfiKfSKYW1dBR2+O9p7+cociIlIWsU8E8+p0CamIxFvsE8GccC+BrhwSkbiKfSIYuKlM9xKISFwpEdQNlAh0CamIxFPsE0FFOsnMqrRKBCISWyVLBGZ2vZntMrNHh5n+BjN72MweMbN7zeyUUsUymuheArURiEg8lbJEcANw3gjTnwLOcveTgH8BrithLCOK7iVQiUBE4qlkicDd7wFaR5h+r7vvCYP3AQtKFcto5s6oYLvuLhaRmJosbQSXAbcPN9HMLjez1Wa2urm5edxXPqeugt2dvfTnC+O+bBGRya7sicDMXkaUCD403Dzufp27r3T3lU1NTeMew7y6Ctz1pDIRiadRE4GZvcjMqsPrN5rZF83s6PFYuZmdDPw/4EJ33z0eyzwc++8lUPWQiMTQWEoE/wF0hat63gc8Cdx4pCs2s0XALcDfuvv6I13ekZg3M0oE2/bqXgIRiZ/UGObJubub2YXAV9z9P83sstHeZGY3AWcDjWa2Bfg4kAZw968BVwENwFfNbGA9Kw9vM47M/JmVAGzZo0QgIvEzlkTQbmZXAm8EXmpmCcIBfSTufsko098KvHVMUZZYbUWamVVptuzpKncoIiITbixVQxcBvcBl7r6D6DLPz5U0qjJYWF+lEoGIxNKYSgTA1e6eN7PjgBXATaUNa+ItqK9k3c72cochIjLhxlIiuAfImtl84E7gb4nuGp5WFtRXsnVPN+5e7lBERCbUWBKBuXsX8Grgq+7+N8CJpQ1r4i2cVUVvrkBzh+4lEJF4GVMiMLMzgTcAPz+E900pC+qjK4c2t6qdQETiZSwH9PcCVwI/dve1ZrYUuKu0YU28BfVVALpySERiZ9TGYndfBawysxozq3H3jcC7Sx/axBooEejKIRGJm7F0MXGSmf0RWAs8ZmZrzOw5pQ9tYlVlUjRUZ1QiEJHYGUvV0NeBf3L3o919EVE3E98obVjlsaC+UiUCEYmdsSSCanff3ybg7ncD1SWLqIwWzNJNZSISP2NJBBvN7J/NbHH4+xiwsdSBlcPAvQSFgu4lEJH4GEsi+Dugiain0B8BjcBbShlUuSyor6IvX9BzCUQkVsZy1dAeBl0lZGbfJ+qDaFpZuP/KoS7m1lWUORoRkYlxuDeGnTmuUUwSC2dF9xI8vVtXDolIfEy7O4SPxML6KhIGT+/uLHcoIiITZtiqITM7dbhJjOF5BFNRJpVgfn0lT6lEICIxMlIbwRdGmPan8Q5ksljcUM2mFpUIRCQ+hk0E7v6yiQxksljSWM2PH9iKuxMeoSkiMq2pjWCQxQ3VtPfm2N3ZV+5QREQmhBLBIEsao5umVT0kInGhRDDI4pAInlIiEJGYGDYRmNkbi16/aNC0d5UyqHJaUF9JMmFs0iWkIhITI5UI/qno9b8PmvZ3JYhlUkgnEyysr2RTiy4hFZF4GCkR2DCvhxqeVhY3VqtqSERiY6RE4MO8Hmr4IGZ2vZntMrNHh5luZnaNmW0ws4dHuIFtwi1uqGbT7k7c1QupiEx/IyWCFeEA/UjR64Hh5WNY9g3AeSNMPx84NvxdDvzHGGMuuSWN1XT15WlWL6QiEgMj3Vl8/JEs2N3vMbPFI8xyIXCjR6fd95nZTDOb5+7bj2S946H4yqHZM9QLqYhMb8OWCNz96eI/oAM4FWgMw0dqPrC5aHhLGHcQM7vczFab2erm5uZxWPXIloZEsFHtBCISAyNdPvozMzsxvJ4HPEp0tdC3zey9ExQfAO5+nbuvdPeVTU1NJV/f/JmVVGWSrN/ZXvJ1iYiU20htBEvcfaCh9y3AL939lcALGJ/LR7cCC4uGF4RxZZdIGMfOqVUiEJFYGCkR9Be9Phe4DcDd24HCOKz7VuBN4eqhM4C2ydA+MOC42TWs29FR7jBEREpupMbizWZ2BVHd/anALwDMrJIxPI/AzG4CzgYazWwL8PGB97n714gSywXABqCLSfYc5OVza/nBmi3s7uiloSZb7nBEREpmpERwGfAp4M+Ai9x9bxh/BvDN0Rbs7peMMt2Bd44xzgl33JxaANbv7OBMJQIRmcZGeh7BLuDtQ4y/C7irlEFNBsvnDiSCds48pqHM0YiIlM5Ij6q8daQ3uvurxj+cyWN2bZa6yjTr1GAsItPcSFVDZxJd538T8Humef9Cg5kZy+fUsn6HEoGITG8jXTU0F/gIcCJwNfByoMXdV7n7qokIrtyOm1vDup3t6nNIRKa1ke4szrv7L9z9UqIG4g3A3dP5WQSDLZ9TS3tPjh37esodiohIyYxUNYSZZYG/BC4BFgPXAD8ufViTw8CVQ+t2tDOvrrLM0YiIlMZIjcU3ElUL3QZ8sugu49hYMXcGAGu37ePs5bPLHI2ISGmMVCJ4I9AJvAd4t9n+tmIjug1gRoljK7u6qjTHNFXzwNN7yh2KiEjJjHQfgR5sD6w8ehZ3PLYDd6coGYqITBs62I/i+UfXs7erX11Si8i0pUQwilOPrgdgjaqHRGSaUiIYxdLGamqzKR7Z0lbuUERESkKJYBSJhHHCUTN4ZKsSgYhMT0oEY3DS/Doe376PXH48HsMgIjK5KBGMwYnz6+jNFdjQrAfViMj0o0QwBifOrwNQO4GITEtKBGOwtLGauso0qzfpyiERmX6UCMYgkTBesGQW9z21u9yhiIiMOyWCMTpjaQNP7+5i297ucociIjKulAjG6Iyl0eMq79uoUoGITC9KBGO0Ym4tdZVpJQIRmXaUCMZofzvBxtZyhyIiMq6UCA7Bmcc08ExrF1vVTiAi00hJE4GZnWdm68xsg5l9eIjpi8zsLjP7o5k9bGYXlDKeI7W/neBJVQ+JyPRRskRgZkngWuB84ATgEjM7YdBsHwNudvfnARcDXy1VPONh+ZxaZlapnUBEppdSlghOBza4+0Z37wO+B1w4aB4HBp50VgdsK2E8R0z3E4jIdFTKRDAf2Fw0vCWMK/YJ4I1mtoXo2chXDLUgM7vczFab2erm5uZSxDpmZyxtYHNrN1v2dJU1DhGR8VLuxuJLgBvcfQFwAfBtMzsoJne/zt1XuvvKpqamCQ+y2EA7we/UTiAi00QpE8FWYGHR8IIwrthlwM0A7v47oAJoLGFMR2z5nFoaazL8zxMt5Q5FRGRclDIR3A8ca2ZLzCxD1Bh866B5ngHOBTCz44kSQXnrfkaRSBhnHTebVeub9XwCEZkWSpYI3D0HvAu4A3ic6OqgtWb2KTN7VZjtfcDbzOwh4Cbgze7upYppvJyzYjZt3f38cfPecociInLEUqVcuLvfRtQIXDzuqqLXjwEvKmUMpfCS4xpJJ41bHtjKaYtnlTscEZEjUu7G4ilpRkWa15++iJtXb+ZJPbVMRKY4JYLD9O5zjyWdNG68d1O5QxEROSJKBIepoSbLaYtn8TvdZSwiU5wSwRE485gG1u/soKWjt9yhiIgcNiWCI6CH1YjIdKBEcAROnl9HbTbFbx7fVe5QREQOmxLBEUglE7zyuUdx26Pb2dfTX+5wREQOixLBEbr4tIX09Bf4yR8H954hIjI1KBEcoZPm1/H8o+u5+ldPsLerr9zhiIgcMiWCI2Rm/MuFJ7K3u5+v3v1kucMRETlkSgTj4ISjZnDuitn85I9byRcmfVdJIiIHUCIYJ6885Sh2tfdy/6bWcociInJIlAjGybnHz6Yqk+Tm+zePPrOIyCSiRDBOqjIp3vCCRfzkwa08sbO93OGIiIyZEsE4esfZy6jKpPj8nevKHYqIyJgpEYyjWdUZ3vaSpdyxdicP6qE1IjJFKBGMs8tesoSG6gyf+ulaCrqCSESmACWCcVaTTXHlBcfzwDN7uen+Z8odjojIqJQISuA1p87ntMX1XPubDXrAvYhMekoEJWBmXP7SY9jW1sMv1u4odzgiIiNSIiiRc1bMZmljNZ+49TE27NLlpCIyeSkRlEgyYVz3ppWYwaXX36+nmInIpKVEUELLZtdw/aWn0dLRyz985wH6cmovEJHJp6SJwMzOM7N1ZrbBzD48zDyvM7PHzGytmX23lPGUw0kL6vjsa0/mD5ta+fTPHyt3OCIiB0mVasFmlgSuBV4ObAHuN7Nb3f2xonmOBa4EXuTue8xsdqniKacLnzufh7e08Z+/fYqzl8/mZSum5WaKyBRVyhLB6cAGd9/o7n3A94ALB83zNuBad98D4O7T9uG/HzxvOSvm1vKBHz5Ma6ceYCMik0cpE8F8oLgrzi1hXLHjgOPM7H/N7D4zO2+oBZnZ5Wa22sxWNzc3lyjc0sqmknzpoueyr7ufv//2arbs6Sp3SCIiQPkbi1PAscDZwCXAN8xs5uCZ3P06d1/p7iubmpomOMTxc/y8GXz2tSfz2LZ9XPT1+/TAexGZFEqZCLYCC4uGF4RxxbYAt7p7v7s/BawnSgzT1l89bz7ffusL2N7WzZU/ekR3HotI2ZUyEdwPHGtmS8wsA1wM3Dponp8QlQYws0aiqqKNJYxpUjh1UT0fOm8FP39kO1fc9Ed6c/lyhyQiMVayRODuOeBdwB3A48DN7r7WzD5lZq8Ks90B7Dazx4C7gA+4++5SxTSZ/P1Zx/DPrziB2x/dwd/dcL8akEWkbMx9anWVvHLlSl+9enW5wxg3P1yzhY/8+BEaqjN85fWn8vyj68sdkohMQ2a2xt1XDjWt3I3Fsffa5y/gR29/Iamk8bqv/45/u+1xlQ5EZEIpEUwCJy2o42dXvIRXP28+X79nI+d84W7uWT81L5MVkalHiWCSqKtM87m/OYU7//GlzJ1RwaXf/ANf+uV6evrVkCwipaVEMMkcN6eWW/7hhfzVc+dz9a+f4JzP380PVm8mr8deikiJqLF4Erv3yRY+c/ufeGhLG7Nrs5yycCaffc3J1Fdnyh2aiEwxaiyeol54TCM/eeeL+OobTuWMpQ2sWt/Ma792L3et26UurUVk3KhEMIXc+2QLH/jBw2zd282MihSXvnAx7zpnGdlUstyhicgkN1KJQIlgiunpz3P3umb++8Gt3P7oDlbMreWMpQ289SVLmDujglRShTwROZgSwTR1+yPb+erdT7JuZzt9uQLJhHHKgjre+2fHkUkleP7R9aSVGEQEJYJpb3NrFz99eBv7unPc+uBWtrX1ADB3RgUvXNbApWcu5pSFB3XqKiIxokQQI23d/fzqsZ2kUwl++tA2fr9xN/t6cixuqOI58+s4Y2kDy2ToCsgAAA1uSURBVJpqeN6imVSk1bYgEhdKBDHW3tPPLQ9s5bcbWli7tW1/aaE2m2JuXQXLZtfwylOOoqc/z4q5M1g+t5ZkwsoctYiMNyUCAcDd2dzazRO72vnNn3bR3N7LQ1v2snNf7/55EganLJzJ8fNmkEoYSxurOWlBHcuaaqmrSpcxehE5EiMlgpI9vF4mHzNjUUMVixqqOPf4OQD05wvc++RuGqozrN3WxlMtXdy3cTd3PLqDvnyB9p7c/vc31mRY2lRDb3+eRQ3VvPCYBhqqMzTUZGmsif5XZ5KYqUQhMpUoEcRcOpngrOOix3+eOL/uoOlb9nSxbkc7TzZ3sGFXBxubO6mpSHHP+mZ++tC2g+ZvrMmybHY1VZkU/fkCmWSCE46aQWUmSTaV5CXHNlKVSdLW3U97T44T59dRk9XXUKSc9AuUES2or2JB/bMliAH9+QItHb3s7uijOfxv6ehl3Y52tu7pZld7D8lEgu6+HHet28VwXSUlDObVVQJgBjXZFEfNrCRXcBprMhzTVENPf57aihS1FWmSCSObSrBi7gxqK1JUpJNkUwmyqYTuoRA5TEoEcljSyQTz6ir3H8RH0tOfpy9fYNe+XtZua6O3v0BFJkltRYoHn9nL5tYuMMBhb3c/29t6yCSNx7bt45YHBj/mengDSaIqk2RuXQX7uqNqrVnVGRprsmTTCZJmJAwSCaM6k6KuMk02lSCZNBJm7OnsY0Zlms7eHE21WWZVZ2jvyVFflaG+Og0OlZkkjTVZevrzmMGcGRV09uZp7ewjX3AWzqpkZlWGtq5+tu/rZnZtBfVVadyhuz9PVSa6WktVaDJZKBFIyVWkk1Skk8yoSLNsds0B0162fPaw73N3uvqiA2dHb472nhwFdzp6c6zb0U53X57eXIHeXJ6e/uh/b3+Bzr4cW/f2sLSxBjNo7exjy54u+vIF3CFfcPIFp7Mvx77u/gNKK5lkgr58dHPekfT4mk0l9q9vYLnJhNEdkkc2lWBhfRV7uvpJGKQSRjJppBLRfKmEDfofxiefHZ9KRAmspy9PNp2grbufo+oqqc6mKHi0jQP/8wXoyxeoSCVo78lRU5EiadHyEmZUZpL7k2I6GS2rszdHOpkgE8alk0ZfvkCh4CycVUVrZx+bWjppqMmSTiaozibZua+HXME5Yd4MzIy27n6yyQSOs7uzj2wqSWU6SVUmGS5fdrbt7WHZ7Ghf7WjrIZtOUp1J0tUXfVbpZIJMMorBDJ7Y2c68mZVUpJN09uaozCSZVZUhk0qwu6OPbW3dLKyvYkF9Jet3ttPVl6euMr3/ryeXp7M3WnbCjHyhQFdfntqKNA3VGfb19JMvODXZFI21WR7d0saWvd2cumgmSxprWLutjWdau5hXV0EykaAmm2JWdYY/bd/Hsjk19OUK7Ons55nWLlq7+ljWVENXX45ls2twh+aOXo6aWUlPf549nX04sLC+iid2tVNbkaa3P09TbZYdbT1gMLMyw8yqKPaqErXBKRHIpGVmVIf2g9qKNLUVz161tGLujHFZh4cDZS4kh8p0kvbeHNlUgh1tPfTlC9RVpmnp6N3fcN7Vl6OlvY90ysjlnT1dfdRk08ysSpMwY8ueLprbe6nOpljcWM3ujl527uulL1egqTZLZ2+Ojt4cW/d2s7ImCzi5/IFx5AqFA4fD9N5cnnzB6c9H8+TyTkU6SW8uT01FmrvWNdOby5NMWFT6Cf+TCSOdjBJRdTZFR9iW/nwBh/1JtVg2laA/Xxi2Wg+gOpOks+/ZZ2YMrLcvPz07RRy8vRPt8pcu5SMXHD/uy1UikFizcFZc3G9fXWWUcBY3Vu8fN2dGxUSHNqHco+TSk8vTlytQk03tv+EwSjwF+vIF0onorHzLni6SiQSLG6pCgirQ2ZujoSZLf74QVfcBMyrTUbJxaKjJ0J9zuvvzdPXl6O7PUyjAnBlZnmzu3N9e1JePztirMkkc6MsV6M8XouSXL7C4sZrtbT2AU5VJ0d2fp7Wjj/58gZlVGebVVbB+Zzt7uvo4uqGahuoMe7v7aevqZ293P9lUghmVadwd96iasCqTpKW9l309/dRVZkgljNauPvZ09nFMUw1LmqpZta6ZJ5s7OH3JLJbNrqG5vTc6w2/vZcueLk5eMJPNe7qiKseqNAvrq5hRmeKJXR3UZFNsbO4gmUjQUJ1hW1s3NdkUM6syFNx5clcHx86ppbc/T2Umyfa9PcytqyCVNPZ197O3q5+27n5OWnDwBR3jQfcRiIjEgJ5HICIiw1IiEBGJOSUCEZGYK2kiMLPzzGydmW0wsw+PMN9rzMzNbMj6KxERKZ2SJQIzSwLXAucDJwCXmNkJQ8xXC7wH+H2pYhERkeGVskRwOrDB3Te6ex/wPeDCIeb7F+AzQE8JYxERkWGUMhHMBzYXDW8J4/Yzs1OBhe7+85EWZGaXm9lqM1vd3Nw8/pGKiMRY2RqLzSwBfBF432jzuvt17r7S3Vc2NTWVPjgRkRgp5Z3FW4GFRcMLwrgBtcCJwN2h74y5wK1m9ip3H/aOsTVr1rSY2dOHEU8j0HIY75uMtC2Tk7ZlctK2RI4ebkLJ7iw2sxSwHjiXKAHcD7ze3dcOM//dwPtHSgJHGM/q4e6qm2q0LZOTtmVy0raMrmRVQ+6eA94F3AE8Dtzs7mvN7FNm9qpSrVdERA5NSTudc/fbgNsGjbtqmHnPLmUsIiIytDjdWXxduQMYR9qWyUnbMjlpW0Yx5XofFRGR8RWnEoGIiAxBiUBEJOZikQjG2vndZGVmm8zsETN70MxWh3GzzOyXZvZE+F9f7jiHYmbXm9kuM3u0aNyQsVvkmrCfHg53nk8aw2zLJ8xsa9g3D5rZBUXTrgzbss7M/qI8UR/MzBaa2V1m9piZrTWz94TxU26/jLAtU3G/VJjZH8zsobAtnwzjl5jZ70PM3zezTBifDcMbwvTFh73y6HFt0/cPSAJPAkuBDPAQcEK54zrEbdgENA4a91ngw+H1h4HPlDvOYWJ/KXAq8OhosQMXALcDBpwB/L7c8Y9hWz5BdP/L4HlPCN+1LLAkfAeT5d6GENs84NTwupbofp8TpuJ+GWFbpuJ+MaAmvE4TdcR5BnAzcHEY/zXgHeH1PwBfC68vBr5/uOuOQ4lgrJ3fTTUXAt8Kr78F/FUZYxmWu98DtA4aPVzsFwI3euQ+YKaZzZuYSEc3zLYM50Lge+7e6+5PARuIvotl5+7b3f2B8Lqd6D6f+UzB/TLCtgxnMu8Xd/eOMJgOfw6cA/wwjB+8Xwb21w+Bcy1003Co4pAIRu38bgpw4E4zW2Nml4dxc9x9e3i9A5hTntAOy3CxT9V99a5QZXJ9URXdlNiWUJ3wPKKzzym9XwZtC0zB/WJmSTN7ENgF/JKoxLLXoxt04cB4929LmN4GNBzOeuOQCKaDF7v7qUTPdninmb20eKJHZcMpeR3wVI49+A/gGOC5wHbgC+UNZ+zMrAb4EfBed99XPG2q7ZchtmVK7hd3z7v7c4n6ZjsdWDER641DIhit87tJz923hv+7gB8TfUF2DhTPw/9d5YvwkA0X+5TbV+6+M/x4C8A3eLaaYVJvi5mliQ6c/+Xut4TRU3K/DLUtU3W/DHD3vcBdwJlEVXEDvUAUx7t/W8L0OmD34awvDongfuDY0PKeIWpUubXMMY2ZmVVb9BQ3zKwa+HPgUaJtuDTMdinw3+WJ8LAMF/utwJvCVSpnAG1FVRWT0qC68r8m2jcQbcvF4cqOJcCxwB8mOr6hhHrk/wQed/cvFk2acvtluG2ZovulycxmhteVwMuJ2jzuAl4bZhu8Xwb212uB34SS3KErd0v5RPwRXfWwnqi+7aPljucQY19KdJXDQ8DagfiJ6gJ/DTwB/AqYVe5Yh4n/JqKieT9R/eZlw8VOdNXEtWE/PQKsLHf8Y9iWb4dYHw4/zHlF8380bMs64Pxyx18U14uJqn0eBh4MfxdMxf0ywrZMxf1yMvDHEPOjwFVh/FKiZLUB+AGQDeMrwvCGMH3p4a5bXUyIiMRcHKqGRERkBEoEIiIxp0QgIhJzSgQiIjGnRCAiEnNKBCKDmFm+qNfKB20ce6w1s8XFvZeKTAYlfWaxyBTV7dFt/iKxoBKByBhZ9FyIz1r0bIg/mNmyMH6xmf0mdHD2azNbFMbPMbMfh/7lHzKzF4ZFJc3sG6HP+TvDXaQiZaNEIHKwykFVQxcVTWtz95OArwBfDuP+HfiWu58M/BdwTRh/DbDK3U8heo7B2jD+WOBad38OsBd4TYm3R2REurNYZBAz63D3miHGbwLOcfeNoaOzHe7eYGYtRF0Y9Ifx29290cyagQXu3lu0jMXAL9392DD8ISDt7v9a+i0TGZpKBCKHxod5fSh6i17nUVudlJkSgcihuajo/+/C63uJerUFeAPwP+H1r4F3wP4HjtRNVJAih0JnIiIHqwxPiRrwC3cfuIS03sweJjqrvySMuwL4ppl9AGgG3hLGvwe4zswuIzrzfwdR76Uik4raCETGKLQRrHT3lnLHIjKeVDUkIhJzKhGIiMScSgQiIjGnRCAiEnNKBCIiMadEICISc0oEIiIx9/8BPyyY050s35MAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}